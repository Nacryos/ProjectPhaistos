# FILE DUMP

## Tree
- README.md
- Makefile
- environment.yml
- requirements.txt
- third_party/LOCK.json
- datasets/.gitkeep
- datasets/__init__.py
- datasets/filters.py
- datasets/registry.py
- repro/.gitkeep
- repro/__init__.py
- repro/data/.gitkeep
- repro/data/__init__.py
- repro/data/prepare.py
- repro/experiments/.gitkeep
- repro/experiments/__init__.py
- repro/experiments/gothic.py
- repro/experiments/iberian.py
- repro/experiments/smoke.py
- repro/experiments/ugaritic.py
- repro/model/.gitkeep
- repro/model/__init__.py
- repro/model/phonetic_prior.py
- repro/paths.py
- repro/reference/.gitkeep
- repro/reference/__init__.py
- repro/reference/paper_metrics.py
- repro/report.py
- repro/third_party.py
- repro/utils.py
- repro/wrappers/.gitkeep
- repro/wrappers/__init__.py
- repro/wrappers/neurodecipher_wrapper.py
- repro/wrappers/xib_wrapper.py
- scripts/eval_gothic.sh
- scripts/fetch_data.sh
- scripts/fig4.sh
- scripts/prepare_datasets.sh
- scripts/reproduce_all.sh
- scripts/run_neurocipher.sh
- scripts/setup.sh
- scripts/smoke_test.sh
- scripts/table2.sh
- scripts/table3.sh
- scripts/table4.sh
- scripts/train_gothic.sh
- artifacts/table2.csv
- artifacts/table3.csv
- artifacts/table4.csv
- artifacts/REPRO_SUMMARY.md
- artifacts/data_provenance.json
- artifacts/PREPARED_DATA_INDEX.txt
- artifacts/runs/gothic_table2.json
- artifacts/runs/gothic_table3_gothic.json
- artifacts/runs/gothic_table4.json
- artifacts/runs/iberian_fig4.json
- artifacts/runs/neurodecipher/metrics.json
- artifacts/runs/neurodecipher/run.log
- artifacts/runs/smoke_test.json
- artifacts/runs/ugaritic_table3.json
- artifacts/figures/.gitkeep
- artifacts/figures/fig4a.png
- artifacts/figures/fig4b.png
- artifacts/figures/fig4c.png
- artifacts/figures/fig4d.png

## Contents

===== README.md =====
# Reproduction: Deciphering Undersegmented Ancient Scripts Using Phonetic Prior

This directory contains a reproducible scaffold for the TACL 2021 paper:

- Paper PDF used locally: `/Users/aaronbao/Downloads/DecipherUnsegmented (3).pdf`
- Upstream paper repo: `third_party/DecipherUnsegmented`
- Baseline repo: `third_party/NeuroDecipher`
- Dataset repo (including religious subset): `third_party/ancient-scripts-datasets`

Pinned repository commits are recorded in `third_party/LOCK.json`.

## Folder layout

```text
repro_decipher_phonetic_prior/
├── README.md
├── Makefile
├── environment.yml
├── requirements.txt
├── artifacts/
├── data/
│   └── prepared/
├── data_external/
├── datasets/
│   ├── __init__.py
│   ├── filters.py
│   └── registry.py
├── repro/
│   ├── __init__.py
│   ├── paths.py
│   ├── utils.py
│   ├── third_party.py
│   ├── report.py
│   ├── data/
│   │   ├── __init__.py
│   │   └── prepare.py
│   ├── experiments/
│   │   ├── __init__.py
│   │   ├── smoke.py
│   │   ├── gothic.py
│   │   ├── ugaritic.py
│   │   └── iberian.py
│   ├── model/
│   │   ├── __init__.py
│   │   └── phonetic_prior.py
│   ├── reference/
│   │   ├── __init__.py
│   │   └── paper_metrics.py
│   └── wrappers/
│       ├── __init__.py
│       ├── neurodecipher_wrapper.py
│       └── xib_wrapper.py
├── scripts/
│   ├── setup.sh
│   ├── fetch_data.sh
│   ├── prepare_datasets.sh
│   ├── smoke_test.sh
│   ├── train_gothic.sh
│   ├── eval_gothic.sh
│   ├── run_neurocipher.sh
│   ├── fig4.sh
│   ├── table2.sh
│   ├── table3.sh
│   ├── table4.sh
│   └── reproduce_all.sh
├── patches/
└── third_party/
    ├── LOCK.json
    ├── DecipherUnsegmented/
    ├── NeuroDecipher/
    ├── ancient-scripts-datasets/
    ├── xib/
    ├── dev_misc/
    └── arglib/
```

## Environment setup

### Conda

```bash
conda env create -f environment.yml
conda activate repro_decipher_phonetic_prior
bash scripts/setup.sh
```

### Pip only

```bash
python3 -m venv .venv
source .venv/bin/activate
bash scripts/setup.sh
```

## Data preparation

### Prepare all corpora + religious variants

```bash
bash scripts/prepare_datasets.sh
```

This writes deterministic prepared datasets under:

- `data/prepared/<corpus>/<variant>/<hash>/`

and provenance to:

- `artifacts/data_provenance.json`

### External appendix assets

```bash
bash scripts/fetch_data.sh
```

`fetch_data.sh` caches downloads in `data_external/` and writes SHA256 checksums to `data_external/checksums.sha256`.
For non-machine-readable appendix assets (Wiktionary descendant-tree extracts, Iberian personal-name TSV), the script fails with explicit required filenames unless `ALLOW_INCOMPLETE=1` is set.

## Single-command pipeline

```bash
bash scripts/reproduce_all.sh
```

Default behavior:

- Runs deterministic data prep.
- Runs a smoke test of DP code paths.
- Runs Gothic/Ugaritic/Iberian experiment wrappers.
- Generates CSV tables and Figure 4 panel images.
- Writes `artifacts/REPRO_SUMMARY.md`.

## Make targets

```bash
make setup
make data
make smoke
make train_gothic
make eval_gothic
make fig4
make table2
make table3
make table4
make reproduce_all
```

## Paper component mapping to code modules

Algorithm 1 mapping:

- `ComputeCharDistr` -> `repro/model/phonetic_prior.py:ComputeCharDistr`
- `EditDistDP` -> `repro/model/phonetic_prior.py:EditDistDP`
- `WordBoundaryDP` -> `repro/model/phonetic_prior.py:WordBoundaryDP`
- SGD step -> `repro/model/phonetic_prior.py:train_one_step`

Model pieces from paper:

- Eq.3 softmax temperature mapping -> `PhoneticPriorModel.compute_char_distr`
- Eq.4 lost embedding composition from known IPA features -> `PhoneticPriorModel.compute_char_distr`
- Monotonic edit-distance DP with insertion/deletion/substitution and adjacent-index insertion -> `PhoneticPriorModel.edit_distance_dp`
- Objective with quality, coverage, sound-loss regularization -> `PhoneticPriorModel.objective`

Upstream wrappers (minimal upstream edits):

- NeuroCipher baseline execution -> `repro/wrappers/neurodecipher_wrapper.py`
- xib extraction wrapper -> `repro/wrappers/xib_wrapper.py`

## Dataset registry API

Implemented in `datasets/registry.py`:

- `list_corpora() -> List[str]`
- `get_corpus(name: str, variant: Optional[str] = None) -> Corpus`
- `export_for_decipherunsegmented(corpus, out_dir, config) -> Dict[str, str]`

`Corpus` exposes:

- `lost_text`
- `known_text`
- `metadata`
- `splits`
- `ground_truth` (when available)

Religious subset variants are available but not used by default targets:

- `get_corpus("gothic", variant="religious")`
- `get_corpus("ugaritic", variant="religious")`
- `get_corpus("iberian", variant="religious")`

## Future: secondary filters

Placeholder filters exist in `datasets/filters.py`:

- `filter_by_religious_subset(corpus: Corpus) -> Corpus`
- `filter_by_metadata(corpus: Corpus, predicate) -> Corpus`

These intentionally raise `NotImplementedError` for now.

To use religious data in future runs, wire a config flag such as `use_religious_variant=true` into the experiment entrypoints and call `get_corpus(..., variant="religious")` during data preparation. `scripts/reproduce_all.sh` does not enable this flag.

## Generated outputs

After a successful run:

- `artifacts/table2.csv`
- `artifacts/table3.csv`
- `artifacts/table4.csv`
- `artifacts/figures/fig4a.png`
- `artifacts/figures/fig4b.png`
- `artifacts/figures/fig4c.png`
- `artifacts/figures/fig4d.png`
- `artifacts/REPRO_SUMMARY.md`
- `artifacts/data_provenance.json`
- `artifacts/runs/smoke_test.json`

## Runtime expectations

- Smoke test: ~1-3 minutes on CPU.
- Reference-mode table/figure generation: <5 minutes.
- Full run-based reproduction with all external assets and upstream training: multi-hour to day-scale CPU runtime (GPU optional).


===== Makefile =====
PYTHON ?= python3

.PHONY: setup data fetch_data smoke train_gothic eval_gothic fig4 table2 table3 table4 report reproduce_all

setup:
	bash scripts/setup.sh

data:
	bash scripts/prepare_datasets.sh

fetch_data:
	bash scripts/fetch_data.sh

smoke:
	bash scripts/smoke_test.sh

train_gothic:
	bash scripts/train_gothic.sh

eval_gothic:
	bash scripts/eval_gothic.sh

fig4:
	bash scripts/fig4.sh
	$(PYTHON) -m repro.report

table2:
	bash scripts/table2.sh

table3:
	bash scripts/table3.sh

table4:
	bash scripts/table4.sh

report:
	$(PYTHON) -m repro.report

reproduce_all:
	bash scripts/reproduce_all.sh

===== environment.yml =====
name: repro_decipher_phonetic_prior
channels:
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - python=3.10
  - pip=24.2
  - setuptools=73.0.1
  - wheel=0.44.0
  - pytorch=2.2.2
  - cpuonly
  - pip:
      - -r requirements.txt

===== requirements.txt =====
numpy==1.26.4
pandas==2.2.3
matplotlib==3.8.4
tqdm==4.66.5
pypdf==4.3.1
requests==2.32.3
pyyaml==6.0.2
prettytable==3.11.0
colorlog==6.8.2
enlighten==1.12.4
pytrie==0.4.0
networkx==3.2.1
Cython==3.0.10
tensorboard==2.17.1
typeguard==4.4.1
inflection==0.5.1
ipapy==0.0.9.0
nltk==3.9.1
lxml==5.2.2
ortools==9.10.4067
cvxopt==1.3.2
treelib==1.7.0
torch==2.2.2

===== third_party/LOCK.json =====
{
  "generated_at": "2026-02-16T01:59:43Z",
  "workspace": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior",
  "repos": [
    {
      "name": "DecipherUnsegmented",
      "url": "https://github.com/j-luo93/DecipherUnsegmented.git",
      "commit": "3cdd0f42572a6af5f4c386a09f322d8fbf3b8347",
      "commit_date": "2021-07-06T21:47:37-04:00",
      "subject": "readme for iberian data",
      "required": true
    },
    {
      "name": "NeuroDecipher",
      "url": "https://github.com/j-luo93/NeuroDecipher.git",
      "commit": "480bad2487820e3737fecfdd108214efa769e34b",
      "commit_date": "2021-09-20T17:14:17-04:00",
      "subject": "updated README.md",
      "required": true
    },
    {
      "name": "ancient-scripts-datasets",
      "url": "https://github.com/Nacryos/ancient-scripts-datasets.git",
      "commit": "36cf764fa8d9cda9a82573d3cf2245317ebac6e4",
      "commit_date": "2026-02-15T15:50:30-08:00",
      "subject": "Initial commit: Ancient Scripts Decipherment Datasets",
      "required": true
    },
    {
      "name": "xib",
      "url": "https://github.com/j-luo93/xib.git",
      "commit": "db3c8a3def0ff47bf3bb2bccf9496fb507bbde07",
      "commit_date": "2019-12-17T02:52:47-05:00",
      "subject": "cleanup 10",
      "required": false
    },
    {
      "name": "dev_misc",
      "url": "https://github.com/j-luo93/dev_misc.git",
      "commit": "b712c8b21beb0ca9f23a4bcd428cfbfa2f703c77",
      "commit_date": "2021-04-25T22:23:47-04:00",
      "subject": "`pad_to_dense` supports 3d tensor",
      "required": false
    },
    {
      "name": "arglib",
      "url": "https://github.com/j-luo93/arglib.git",
      "commit": "6662216e7192fb87c1d04f53b074b538d7cd7df1",
      "commit_date": "2019-11-17T16:40:01-05:00",
      "subject": "decorators",
      "required": false
    }
  ]
}

===== datasets/.gitkeep =====


===== datasets/__init__.py =====
"""Dataset registry for decipherment reproduction."""

from .registry import Corpus, export_for_decipherunsegmented, get_corpus, list_corpora

__all__ = [
    "Corpus",
    "list_corpora",
    "get_corpus",
    "export_for_decipherunsegmented",
]

===== datasets/filters.py =====
"""Placeholder filter hooks for future corpus slicing."""

from __future__ import annotations

from datasets.registry import Corpus


def filter_by_religious_subset(corpus: Corpus) -> Corpus:
    raise NotImplementedError("Religious subset filtering is not implemented yet.")


def filter_by_metadata(corpus: Corpus, predicate) -> Corpus:
    raise NotImplementedError("Metadata filtering is not implemented yet.")

===== datasets/registry.py =====
"""Stable corpus registry for all data sources used in this replication."""

from __future__ import annotations

import ast
import csv
import hashlib
import json
import re
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional


@dataclass(frozen=True)
class Corpus:
    name: str
    variant: Optional[str]
    lost_text: List[str]
    known_text: Dict[str, List[str]]
    metadata: Dict[str, Any]
    splits: Dict[str, List[int]]
    ground_truth: Optional[List[Dict[str, Any]]] = None


_PROJECT_ROOT = Path(__file__).resolve().parents[1]
_THIRD_PARTY = _PROJECT_ROOT / "third_party"

_CANONICAL_SOURCES = {
    "gothic": "ancient-scripts-datasets",
    "ugaritic": "NeuroDecipher",
    "iberian": "DecipherUnsegmented",
}

_CORPORA = ("gothic", "ugaritic", "iberian")


def _read_lines(path: Path) -> List[str]:
    return path.read_text(encoding="utf8").splitlines()


def _tokenize_plain_text(lines: Iterable[str]) -> List[str]:
    token_lines: List[str] = []
    for raw in lines:
        line = raw.strip()
        if not line or line.startswith("#"):
            continue
        line = line.lower()
        line = re.sub(r"[^\w\s\-þðƕȝōēæáéíóúïöü]+", " ", line)
        tokens = [tok for tok in line.split() if tok]
        if tokens:
            token_lines.append(" ".join(tokens))
    return token_lines


def _hash_to_bucket(value: str, mod: int) -> int:
    return int(hashlib.sha256(value.encode("utf8")).hexdigest(), 16) % mod


def _deterministic_splits(lost_text: List[str]) -> Dict[str, List[int]]:
    train: List[int] = []
    dev: List[int] = []
    test: List[int] = []
    for idx, line in enumerate(lost_text):
        bucket = _hash_to_bucket(f"{idx}:{line}", 10)
        if bucket <= 7:
            train.append(idx)
        elif bucket == 8:
            dev.append(idx)
        else:
            test.append(idx)
    return {"train": train, "dev": dev, "test": test}


def _read_tsv_with_comments(path: Path) -> List[Dict[str, str]]:
    rows: List[Dict[str, str]] = []
    with path.open("r", encoding="utf8") as fin:
        header: Optional[List[str]] = None
        for line in fin:
            stripped = line.strip()
            if not stripped or stripped.startswith("#"):
                continue
            if header is None:
                header = stripped.split("\t")
                continue
            values = stripped.split("\t")
            if len(values) < len(header):
                values.extend([""] * (len(header) - len(values)))
            row = dict(zip(header, values))
            rows.append(row)
    return rows


def _build_gothic_default() -> Corpus:
    source = _THIRD_PARTY / "ancient-scripts-datasets" / "data" / "gothic"
    gotica_path = source / "gotica.txt"
    lost_text = _tokenize_plain_text(_read_lines(gotica_path))
    metadata = {
        "canonical_source": _CANONICAL_SOURCES["gothic"],
        "source_paths": {
            "gotica": str(gotica_path),
            "pretrained_embedding": str(source / "got.pretrained.pth"),
            "segments": str(source / "segments.pkl"),
        },
        "notes": "Known-language vocabularies (PG/ON/OE) are external and prepared separately.",
    }
    return Corpus(
        name="gothic",
        variant=None,
        lost_text=lost_text,
        known_text={},
        metadata=metadata,
        splits=_deterministic_splits(lost_text),
        ground_truth=None,
    )


def _build_gothic_religious() -> Corpus:
    path = _THIRD_PARTY / "ancient-scripts-datasets" / "data" / "religious_terms" / "gothic_religious.tsv"
    rows = _read_tsv_with_comments(path)
    lost = [row.get("gothic_word", "").strip() for row in rows if row.get("gothic_word", "").strip()]
    english = [row.get("english_meaning", "").strip() for row in rows if row.get("gothic_word", "").strip()]
    gt = [
        {
            "lost": row.get("gothic_word", "").strip(),
            "known": row.get("english_meaning", "").strip(),
            "category": row.get("category", "").strip(),
            "subcategory": row.get("subcategory", "").strip(),
        }
        for row in rows
        if row.get("gothic_word", "").strip()
    ]
    metadata = {
        "canonical_source": "ancient-scripts-datasets",
        "source_paths": {"religious_terms": str(path)},
        "is_religious_subset": True,
        "unused_by_default": True,
    }
    return Corpus(
        name="gothic",
        variant="religious",
        lost_text=lost,
        known_text={"english_gloss": english},
        metadata=metadata,
        splits=_deterministic_splits(lost),
        ground_truth=gt,
    )


def _build_ugaritic_default() -> Corpus:
    path = _THIRD_PARTY / "NeuroDecipher" / "data" / "uga-heb.no_spe.cog"
    lost_text: List[str] = []
    hebrew_vocab: List[str] = []
    gt: List[Dict[str, Any]] = []
    with path.open("r", encoding="utf8") as fin:
        reader = csv.DictReader(fin, delimiter="\t")
        for row in reader:
            lost = (row.get("uga-no_spe") or "").strip()
            known_raw = (row.get("heb-no_spe") or "").strip()
            if not lost or not known_raw or known_raw == "_":
                continue
            known_candidates = [tok.strip() for tok in known_raw.split("|") if tok.strip()]
            if not known_candidates:
                continue
            lost_text.append(lost)
            hebrew_vocab.extend(known_candidates)
            gt.append({"lost": lost, "known": known_candidates})
    metadata = {
        "canonical_source": _CANONICAL_SOURCES["ugaritic"],
        "source_paths": {"cognates": str(path)},
        "known_column": "heb-no_spe",
        "lost_column": "uga-no_spe",
    }
    return Corpus(
        name="ugaritic",
        variant=None,
        lost_text=lost_text,
        known_text={"hebrew": sorted(set(hebrew_vocab))},
        metadata=metadata,
        splits=_deterministic_splits(lost_text),
        ground_truth=gt,
    )


def _build_ugaritic_religious() -> Corpus:
    path = _THIRD_PARTY / "ancient-scripts-datasets" / "data" / "religious_terms" / "ugaritic_hebrew_religious.tsv"
    rows = _read_tsv_with_comments(path)
    lost: List[str] = []
    heb: List[str] = []
    gt: List[Dict[str, Any]] = []
    for row in rows:
        lost_form = row.get("ugaritic_form", "").strip()
        known_raw = row.get("hebrew_cognate", "").strip()
        if not lost_form:
            continue
        candidates = [tok.strip() for tok in known_raw.split("|") if tok.strip()]
        if not candidates:
            candidates = [known_raw] if known_raw else []
        lost.append(lost_form)
        heb.extend(candidates)
        gt.append(
            {
                "lost": lost_form,
                "known": candidates,
                "category": row.get("category", "").strip(),
                "subcategory": row.get("subcategory", "").strip(),
            }
        )
    metadata = {
        "canonical_source": "ancient-scripts-datasets",
        "source_paths": {"religious_terms": str(path)},
        "is_religious_subset": True,
        "unused_by_default": True,
    }
    return Corpus(
        name="ugaritic",
        variant="religious",
        lost_text=lost,
        known_text={"hebrew": sorted(set(heb))},
        metadata=metadata,
        splits=_deterministic_splits(lost),
        ground_truth=gt,
    )


def _build_iberian_default() -> Corpus:
    path = _THIRD_PARTY / "DecipherUnsegmented" / "data" / "iberian.csv"
    lost: List[str] = []
    with path.open("r", encoding="utf8") as fin:
        reader = csv.DictReader(fin)
        for row in reader:
            raw = (row.get("cleaned") or "").strip()
            if not raw:
                continue
            try:
                parsed = ast.literal_eval(raw)
                if isinstance(parsed, list):
                    content = " ".join(str(x) for x in parsed if str(x).strip())
                else:
                    content = str(parsed)
            except (ValueError, SyntaxError):
                content = raw
            content = content.strip()
            if content:
                lost.append(content)
    metadata = {
        "canonical_source": _CANONICAL_SOURCES["iberian"],
        "source_paths": {"iberian_csv": str(path)},
        "notes": "Known vocab for personal-name experiment comes from Rodriguez Ramos (2014).",
    }
    return Corpus(
        name="iberian",
        variant=None,
        lost_text=lost,
        known_text={},
        metadata=metadata,
        splits=_deterministic_splits(lost),
        ground_truth=None,
    )


def _build_iberian_religious() -> Corpus:
    path = _THIRD_PARTY / "ancient-scripts-datasets" / "data" / "religious_terms" / "iberian_religious.tsv"
    rows = _read_tsv_with_comments(path)
    # The header in this file currently starts with "3category".
    first_col = next(iter(rows[0].keys())) if rows else "category"
    lost: List[str] = []
    known: List[str] = []
    gt: List[Dict[str, Any]] = []
    for row in rows:
        element = (row.get("element") or "").strip()
        proposed = (row.get("proposed_meaning") or "").strip()
        if not element:
            continue
        lost.append(element)
        if proposed:
            known.append(proposed)
        gt.append(
            {
                "lost": element,
                "known": proposed,
                "category": (row.get("category") or row.get(first_col) or "").strip(),
                "subcategory": row.get("subcategory", "").strip(),
            }
        )
    metadata = {
        "canonical_source": "ancient-scripts-datasets",
        "source_paths": {"religious_terms": str(path)},
        "is_religious_subset": True,
        "unused_by_default": True,
    }
    return Corpus(
        name="iberian",
        variant="religious",
        lost_text=lost,
        known_text={"proposed_meaning": sorted(set(known))},
        metadata=metadata,
        splits=_deterministic_splits(lost),
        ground_truth=gt,
    )


_BUILDERS = {
    ("gothic", None): _build_gothic_default,
    ("gothic", "religious"): _build_gothic_religious,
    ("ugaritic", None): _build_ugaritic_default,
    ("ugaritic", "religious"): _build_ugaritic_religious,
    ("iberian", None): _build_iberian_default,
    ("iberian", "religious"): _build_iberian_religious,
}


def list_corpora() -> List[str]:
    return list(_CORPORA)


def get_corpus(name: str, variant: Optional[str] = None) -> Corpus:
    key = (name.lower(), variant)
    if key not in _BUILDERS:
        raise ValueError(f"Unsupported corpus/variant combination: {name!r}, {variant!r}")
    corpus = _BUILDERS[key]()
    return corpus


def export_for_decipherunsegmented(
    corpus: Corpus,
    out_dir: Path,
    config: Optional[Dict[str, Any]] = None,
) -> Dict[str, str]:
    config = config or {}
    out = Path(out_dir)
    out.mkdir(parents=True, exist_ok=True)

    lost_path = out / "lost.txt"
    lost_path.write_text("\n".join(corpus.lost_text) + "\n", encoding="utf8")

    known_paths: Dict[str, str] = {}
    for known_name, vocab in corpus.known_text.items():
        kp = out / f"known_{known_name}.txt"
        kp.write_text("\n".join(sorted(set(vocab))) + "\n", encoding="utf8")
        known_paths[known_name] = str(kp)

    split_path = out / "splits.json"
    split_path.write_text(json.dumps(corpus.splits, indent=2) + "\n", encoding="utf8")

    meta = dict(corpus.metadata)
    meta["name"] = corpus.name
    meta["variant"] = corpus.variant
    meta["config"] = config
    meta_path = out / "metadata.json"
    meta_path.write_text(json.dumps(meta, indent=2, sort_keys=True) + "\n", encoding="utf8")

    gt_path: Optional[Path] = None
    if corpus.ground_truth:
        gt_path = out / "ground_truth.tsv"
        all_fields = sorted({k for row in corpus.ground_truth for k in row.keys()})
        with gt_path.open("w", encoding="utf8", newline="") as fout:
            writer = csv.DictWriter(fout, fieldnames=all_fields, delimiter="\t")
            writer.writeheader()
            for row in corpus.ground_truth:
                writer.writerow(row)

    exported = {
        "lost_text": str(lost_path),
        "splits": str(split_path),
        "metadata": str(meta_path),
    }
    if known_paths:
        exported["known_text"] = json.dumps(known_paths, sort_keys=True)
    if gt_path is not None:
        exported["ground_truth"] = str(gt_path)
    return exported


def corpus_fingerprint(corpus: Corpus, extra: Optional[Dict[str, Any]] = None) -> str:
    payload = {
        "corpus": asdict(corpus),
        "extra": extra or {},
    }
    encoded = json.dumps(payload, sort_keys=True, ensure_ascii=False).encode("utf8")
    return hashlib.sha256(encoded).hexdigest()[:16]

===== repro/.gitkeep =====


===== repro/__init__.py =====
"""Replication package for Deciphering Undersegmented Ancient Scripts Using Phonetic Prior."""

===== repro/data/.gitkeep =====


===== repro/data/__init__.py =====
"""Data preparation package."""

===== repro/data/prepare.py =====
"""Deterministic dataset preparation and provenance logging."""

from __future__ import annotations

import argparse
import json
import shutil
from dataclasses import asdict
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from datasets.registry import (
    Corpus,
    corpus_fingerprint,
    export_for_decipherunsegmented,
    get_corpus,
    list_corpora,
)
from repro.paths import ARTIFACTS, DATA_EXTERNAL, DATA_PREPARED, ROOT, THIRD_PARTY
from repro.third_party import load_lock
from repro.utils import sha256_file, utc_now_iso, write_json


EXPERIMENT_REQUIREMENTS = {
    "gothic": [
        "wiktionary_descendants_pg.tsv",
        "wiktionary_descendants_on.tsv",
        "wiktionary_descendants_oe.tsv",
    ],
    "iberian": [
        "rodriguez_ramos_2014_personal_names.tsv",
    ],
    "ugaritic": [],
}


def _candidate_sources(corpus_name: str) -> List[Tuple[str, Path]]:
    if corpus_name == "gothic":
        return [
            ("ancient-scripts-datasets", THIRD_PARTY / "ancient-scripts-datasets" / "data" / "gothic"),
            ("DecipherUnsegmented", THIRD_PARTY / "DecipherUnsegmented" / "data"),
        ]
    if corpus_name == "ugaritic":
        return [
            ("NeuroDecipher", THIRD_PARTY / "NeuroDecipher" / "data"),
            ("ancient-scripts-datasets", THIRD_PARTY / "ancient-scripts-datasets" / "data" / "ugaritic"),
        ]
    if corpus_name == "iberian":
        return [
            ("DecipherUnsegmented", THIRD_PARTY / "DecipherUnsegmented" / "data"),
            ("ancient-scripts-datasets", THIRD_PARTY / "ancient-scripts-datasets" / "data" / "iberian"),
        ]
    return []


def _canonical_source(corpus_name: str) -> str:
    if corpus_name == "gothic":
        return "ancient-scripts-datasets"
    if corpus_name == "ugaritic":
        return "NeuroDecipher"
    if corpus_name == "iberian":
        return "DecipherUnsegmented"
    raise ValueError(f"Unknown corpus {corpus_name}")


def _repo_commit_map() -> Dict[str, Dict[str, str]]:
    lock = load_lock()
    out = {}
    for repo in lock.get("repos", []):
        out[repo["name"]] = {
            "commit": repo["commit"],
            "commit_date": repo["commit_date"],
            "url": repo["url"],
        }
    return out


def _copy_optional_assets(corpus: Corpus, out_dir: Path) -> List[str]:
    copied: List[str] = []
    if corpus.name == "gothic" and corpus.variant is None:
        src_root = THIRD_PARTY / "ancient-scripts-datasets" / "data" / "gothic"
        assets = ["segments.pkl", "got.pretrained.pth", "gotica.xml.zip"]
        assets_dir = out_dir / "assets"
        assets_dir.mkdir(parents=True, exist_ok=True)
        for asset in assets:
            src = src_root / asset
            if src.exists():
                dst = assets_dir / asset
                if not dst.exists():
                    shutil.copy2(src, dst)
                copied.append(str(dst))
    return copied


def prepare_one(corpus_name: str, variant: Optional[str], config: Dict[str, Any]) -> Dict[str, Any]:
    corpus = get_corpus(corpus_name, variant=variant)
    fingerprint = corpus_fingerprint(corpus, extra=config)
    variant_key = variant or "default"
    out_dir = DATA_PREPARED / corpus_name / variant_key / fingerprint
    exported = export_for_decipherunsegmented(corpus, out_dir, config)
    copied_assets = _copy_optional_assets(corpus, out_dir)

    manifest = {
        "corpus": corpus.name,
        "variant": corpus.variant,
        "fingerprint": fingerprint,
        "num_lost_lines": len(corpus.lost_text),
        "num_known_vocab": {k: len(v) for k, v in corpus.known_text.items()},
        "num_ground_truth_rows": len(corpus.ground_truth or []),
        "exported": exported,
        "copied_assets": copied_assets,
        "prepared_at": utc_now_iso(),
    }
    write_json(out_dir / "manifest.json", manifest)

    return {
        "name": corpus.name,
        "variant": corpus.variant,
        "fingerprint": fingerprint,
        "out_dir": str(out_dir),
        "canonical_source": _canonical_source(corpus_name),
        "candidate_sources": [
            {"repo": repo, "path": str(path), "exists": path.exists()} for repo, path in _candidate_sources(corpus_name)
        ],
        "manifest": manifest,
    }


def _missing_external_requirements(corpus_names: List[str]) -> Dict[str, List[str]]:
    missing: Dict[str, List[str]] = {}
    for name in corpus_names:
        needed = EXPERIMENT_REQUIREMENTS.get(name, [])
        not_found = [fname for fname in needed if not (DATA_EXTERNAL / fname).exists()]
        if not_found:
            missing[name] = not_found
    return missing


def _external_checksums() -> Dict[str, str]:
    checksums: Dict[str, str] = {}
    if not DATA_EXTERNAL.exists():
        return checksums
    for path in sorted(DATA_EXTERNAL.glob("**/*")):
        if path.is_file() and path.name != "checksums.sha256":
            checksums[str(path.relative_to(ROOT))] = sha256_file(path)
    return checksums


def main() -> None:
    parser = argparse.ArgumentParser(description="Prepare deterministic datasets for replication.")
    parser.add_argument(
        "--corpora",
        type=str,
        default=",".join(list_corpora()),
        help="Comma-separated corpus names.",
    )
    parser.add_argument(
        "--include-religious-variants",
        action="store_true",
        help="Prepare additional religious variants (not used by default runs).",
    )
    parser.add_argument("--strict", action="store_true", help="Fail if appendix external assets are missing.")
    parser.add_argument("--seed", type=int, default=1234)
    parser.add_argument("--max-lines", type=int, default=0, help="Optional cap for quick debug preparation.")
    args = parser.parse_args()

    selected = [item.strip().lower() for item in args.corpora.split(",") if item.strip()]
    for name in selected:
        if name not in list_corpora():
            raise ValueError(f"Unknown corpus: {name}")

    prep_config = {
        "seed": args.seed,
        "max_lines": args.max_lines,
    }

    prepared: List[Dict[str, Any]] = []
    for name in selected:
        prepared.append(prepare_one(name, variant=None, config=prep_config))
        if args.include_religious_variants:
            prepared.append(prepare_one(name, variant="religious", config=prep_config))

    missing_external = _missing_external_requirements(selected)
    if args.strict and missing_external:
        missing_str = json.dumps(missing_external, indent=2)
        raise SystemExit(
            "Missing appendix/external assets required for full replication. "
            f"Run scripts/fetch_data.sh first.\n{missing_str}"
        )

    provenance = {
        "prepared_at": utc_now_iso(),
        "project_root": str(ROOT),
        "paper_pdf_used": str((Path("/Users/aaronbao/Downloads") / "DecipherUnsegmented (3).pdf")),
        "repos": _repo_commit_map(),
        "prepared": prepared,
        "missing_external_requirements": missing_external,
        "external_cache_checksums": _external_checksums(),
    }
    write_json(ARTIFACTS / "data_provenance.json", provenance)

    print(f"Prepared {len(prepared)} dataset variant(s).")
    print(f"Wrote provenance to {ARTIFACTS / 'data_provenance.json'}")


if __name__ == "__main__":
    main()

===== repro/experiments/.gitkeep =====


===== repro/experiments/__init__.py =====
"""Experiment entrypoints."""

===== repro/experiments/gothic.py =====
"""Gothic experiment driver for Table 2/4 and related metrics."""

from __future__ import annotations

import argparse
from pathlib import Path
from typing import Dict, List

from repro.paths import ARTIFACTS
from repro.reference.paper_metrics import TABLE2, TABLE3, TABLE4
from repro.utils import utc_now_iso, write_json


def run_reference(task: str) -> Dict:
    if task == "table2":
        rows = TABLE2["gothic"]
    elif task == "table4":
        rows = TABLE4
    elif task == "table3_gothic":
        rows = [row for row in TABLE3 if row["lost"] == "Gothic"]
    else:
        raise ValueError(f"Unsupported Gothic task: {task}")

    return {
        "status": "ok",
        "mode": "reference",
        "task": task,
        "created_at": utc_now_iso(),
        "rows": rows,
        "note": "Values are from the paper's reported tables.",
    }


def main() -> None:
    parser = argparse.ArgumentParser(description="Run Gothic experiment wrappers.")
    parser.add_argument("--task", choices=["table2", "table4", "table3_gothic"], required=True)
    parser.add_argument("--mode", choices=["reference", "train"], default="reference")
    args = parser.parse_args()

    if args.mode == "train":
        raise SystemExit(
            "Gothic train mode requires external known-vocabulary assets (PG/ON/OE descendant-tree extracts). "
            "Run scripts/fetch_data.sh and provide converted vocab files, then re-run with wrapper extensions."
        )

    payload = run_reference(args.task)
    out_path = ARTIFACTS / "runs" / f"gothic_{args.task}.json"
    write_json(out_path, payload)
    print(f"Wrote {out_path}")


if __name__ == "__main__":
    main()

===== repro/experiments/iberian.py =====
"""Iberian experiment driver for Figure 4 data."""

from __future__ import annotations

import argparse

from repro.paths import ARTIFACTS
from repro.reference.paper_metrics import FIG4A_PAPER_TRACE, FIG4_CLOSENESS_TRACE
from repro.utils import utc_now_iso, write_json


def run_reference() -> dict:
    return {
        "status": "ok",
        "mode": "reference",
        "created_at": utc_now_iso(),
        "fig4a": FIG4A_PAPER_TRACE,
        "fig4_closeness": FIG4_CLOSENESS_TRACE,
        "note": "Figure points are digitized traces from Figure 4 in the paper.",
    }


def main() -> None:
    parser = argparse.ArgumentParser(description="Prepare Figure 4 data.")
    parser.add_argument("--mode", choices=["reference", "train"], default="reference")
    args = parser.parse_args()

    if args.mode == "train":
        raise SystemExit(
            "Iberian train mode is not yet wired because personal-name correspondences are in a PDF source. "
            "Use reference mode or provide machine-readable gold labels in data_external/."
        )

    payload = run_reference()
    out_path = ARTIFACTS / "runs" / "iberian_fig4.json"
    write_json(out_path, payload)
    print(f"Wrote {out_path}")


if __name__ == "__main__":
    main()

===== repro/experiments/smoke.py =====
"""Fast smoke test for DP code paths and training step."""

from __future__ import annotations

import argparse
from pathlib import Path
from typing import List

import torch

from repro.model import PhoneticPriorConfig, PhoneticPriorModel, train_one_step
from repro.paths import ARTIFACTS
from repro.utils import set_global_seeds, utc_now_iso, write_json


def run_smoke(steps: int = 5, seed: int = 1234) -> Path:
    set_global_seeds(seed)

    inscriptions = [
        "þammuhsaminhaidau",
        "gards",
        "wulfs",
        "sunus",
        "hausjan",
    ]
    known_vocab = ["xaið", "raið", "braið", "gard", "wulf", "sunu", "hausjan"]

    chars_lost = sorted(set("".join(inscriptions)))
    chars_known = sorted(set("".join(known_vocab)))

    cfg = PhoneticPriorConfig(
        temperature=0.2,
        alpha=3.5,
        lambda_cov=1.0,
        lambda_loss=1.0,
        min_span=3,
        max_span=6,
        embedding_dim=16,
        lr=0.2,
        seed=seed,
    )
    model = PhoneticPriorModel(chars_lost, chars_known, config=cfg)
    optimizer = torch.optim.SGD(model.parameters(), lr=cfg.lr)

    history = []
    for step in range(1, steps + 1):
        out = train_one_step(model, optimizer, inscriptions, known_vocab)
        history.append(
            {
                "step": step,
                "objective": out.objective,
                "quality": out.quality,
                "omega_cov": out.omega_cov,
                "omega_loss": out.omega_loss,
                "num_sequences": out.num_sequences,
            }
        )

    result = {
        "status": "ok",
        "created_at": utc_now_iso(),
        "seed": seed,
        "steps": steps,
        "history": history,
    }

    out_path = ARTIFACTS / "runs" / "smoke_test.json"
    write_json(out_path, result)
    return out_path


def main() -> None:
    parser = argparse.ArgumentParser(description="Run a quick smoke test for DP/training code paths.")
    parser.add_argument("--steps", type=int, default=5)
    parser.add_argument("--seed", type=int, default=1234)
    args = parser.parse_args()

    out = run_smoke(steps=args.steps, seed=args.seed)
    print(f"Smoke test written to {out}")


if __name__ == "__main__":
    main()

===== repro/experiments/ugaritic.py =====
"""Ugaritic experiment driver (Table 3 baseline integration)."""

from __future__ import annotations

import argparse
from pathlib import Path
from typing import Dict, List

from repro.paths import ARTIFACTS
from repro.reference.paper_metrics import TABLE3
from repro.utils import utc_now_iso, write_json
from repro.wrappers.neurodecipher_wrapper import run_neurodecipher


def run_reference() -> Dict:
    rows = [row for row in TABLE3 if row["lost"] == "Ugaritic"]
    return {
        "status": "ok",
        "mode": "reference",
        "created_at": utc_now_iso(),
        "rows": rows,
        "note": "Values are from the paper's reported table.",
    }


def run_neuro() -> Dict:
    run = run_neurodecipher(cfg="UgaHebSmallNoSpe", overrides={"num_rounds": "2", "num_epochs_per_M_step": "10"})
    rows = [row for row in TABLE3 if row["lost"] == "Ugaritic"]
    neuro_row = next((row for row in rows if row["method"] == "NeuroCipher"), None)
    return {
        "status": run.get("status", "failed"),
        "mode": "neuro",
        "created_at": utc_now_iso(),
        "rows": rows,
        "neuro_run": run,
        "paper_neuro_score": neuro_row["score"] if neuro_row else None,
        "note": "NeuroDecipher output can differ because this wrapper uses a short run for reproducibility checks.",
    }


def main() -> None:
    parser = argparse.ArgumentParser(description="Run Ugaritic baseline experiment.")
    parser.add_argument("--mode", choices=["reference", "neuro"], default="reference")
    args = parser.parse_args()

    if args.mode == "reference":
        payload = run_reference()
    else:
        payload = run_neuro()

    out_path = ARTIFACTS / "runs" / "ugaritic_table3.json"
    write_json(out_path, payload)
    print(f"Wrote {out_path}")


if __name__ == "__main__":
    main()

===== repro/model/.gitkeep =====


===== repro/model/__init__.py =====
"""Model implementations for reproduction."""

from .phonetic_prior import (
    ComputeCharDistr,
    EditDistDP,
    PhoneticPriorConfig,
    PhoneticPriorModel,
    TrainStepOutput,
    WordBoundaryDP,
    train_one_step,
)

__all__ = [
    "PhoneticPriorConfig",
    "PhoneticPriorModel",
    "TrainStepOutput",
    "ComputeCharDistr",
    "EditDistDP",
    "WordBoundaryDP",
    "train_one_step",
]

===== repro/model/phonetic_prior.py =====
"""Phonetic-prior decipherment model with DP objectives.

This module follows the paper algorithm structure:
1) ComputeCharDistr
2) EditDistDP
3) WordBoundaryDP
4) SGD step
"""

from __future__ import annotations

import math
from dataclasses import dataclass
from typing import Dict, Iterable, List, Optional, Sequence, Tuple

import torch
import torch.nn as nn


Tensor = torch.Tensor


def _softmin(values: Sequence[Tensor]) -> Tensor:
    stacked = torch.stack([-v for v in values], dim=0)
    return -torch.logsumexp(stacked, dim=0)


@dataclass
class PhoneticPriorConfig:
    temperature: float = 0.2
    alpha: float = 3.5
    lambda_cov: float = 10.0
    lambda_loss: float = 100.0
    min_span: int = 4
    max_span: int = 10
    embedding_dim: int = 64
    lr: float = 0.2
    p_o: float = 0.2
    seed: int = 1234


@dataclass
class TrainStepOutput:
    objective: float
    quality: float
    omega_cov: float
    omega_loss: float
    num_sequences: int


class PhoneticPriorModel(nn.Module):
    """Joint segmentation and cognate-alignment model."""

    def __init__(
        self,
        lost_chars: Sequence[str],
        known_chars: Sequence[str],
        known_ipa_features: Optional[Tensor] = None,
        config: Optional[PhoneticPriorConfig] = None,
    ) -> None:
        super().__init__()
        self.config = config or PhoneticPriorConfig()
        self.lost_chars = sorted(set(lost_chars))
        self.known_chars = sorted(set(known_chars))
        if not self.lost_chars:
            self.lost_chars = ["?"]
        if not self.known_chars:
            self.known_chars = ["?"]

        self.lost2idx = {c: i for i, c in enumerate(self.lost_chars)}
        self.known2idx = {c: i for i, c in enumerate(self.known_chars)}

        d = self.config.embedding_dim
        if known_ipa_features is None:
            known_ipa_features = torch.eye(len(self.known_chars), dtype=torch.float32)
        if known_ipa_features.ndim != 2:
            raise ValueError("known_ipa_features must be rank-2")

        feature_dim = int(known_ipa_features.shape[1])
        self.register_buffer("known_ipa_features", known_ipa_features.float())
        self.ipa_projector = nn.Linear(feature_dim, d, bias=False)

        # Eq.3 logits over known/lost character pairs.
        self.mapping_logits = nn.Parameter(torch.zeros(len(self.known_chars), len(self.lost_chars)))
        nn.init.xavier_uniform_(self.mapping_logits)

    def known_embeddings(self) -> Tensor:
        return self.ipa_projector(self.known_ipa_features)

    def compute_char_distr(self) -> Tensor:
        """Eq.3: softmax(dot / T) over lost chars given known chars."""
        k_emb = self.known_embeddings()  # K x D

        # Eq.4: build lost embeddings as weighted sums of known IPA embeddings.
        mix = self.mapping_logits.softmax(dim=0)  # K x L, normalized over known chars.
        l_emb = mix.transpose(0, 1) @ k_emb  # L x D

        scores = (k_emb @ l_emb.transpose(0, 1)) / max(self.config.temperature, 1e-6)  # K x L
        return scores.log_softmax(dim=1).exp()

    def edit_distance_dp(self, lost_token: str, known_token: str, char_distr: Tensor) -> Tensor:
        """Section 3.2.2 monotonic alignment with substitution/deletion/insertion."""
        m = len(lost_token)
        n = len(known_token)
        alpha = torch.tensor(float(self.config.alpha), dtype=torch.float32, device=char_distr.device)

        dp: List[List[Tensor]] = [[torch.tensor(0.0, device=char_distr.device) for _ in range(n + 1)] for _ in range(m + 1)]
        for i in range(1, m + 1):
            dp[i][0] = dp[i - 1][0] + alpha
        for j in range(1, n + 1):
            dp[0][j] = dp[0][j - 1] + alpha

        for i in range(1, m + 1):
            lch = lost_token[i - 1]
            l_idx = self.lost2idx.get(lch, 0)
            for j in range(1, n + 1):
                kch = known_token[j - 1]
                k_idx = self.known2idx.get(kch, 0)

                sub_cost = -torch.log(char_distr[k_idx, l_idx] + 1e-9)
                candidates = [
                    dp[i - 1][j] + alpha,      # deletion
                    dp[i][j - 1] + alpha,      # insertion
                    dp[i - 1][j - 1] + sub_cost,  # substitution
                ]

                # Two-adjacent-index insertion transition.
                if j >= 2:
                    candidates.append(dp[i][j - 2] + alpha)
                dp[i][j] = _softmin(candidates)

        final_cost = dp[m][n]
        return -final_cost

    def word_boundary_dp(
        self,
        inscription: str,
        known_vocab: Sequence[str],
        char_distr: Tensor,
    ) -> Tuple[Tensor, float]:
        """Section 3.3 objective over latent Z with O and E_l tags."""
        seq = inscription.replace(" ", "")
        n = len(seq)
        if n == 0:
            return torch.tensor(0.0, device=char_distr.device), 0.0

        min_span = max(1, self.config.min_span)
        max_span = max(min_span, self.config.max_span)

        p_o = max(min(self.config.p_o, 0.99), 0.01)
        log_p_o = math.log(p_o)
        remaining = max(1, max_span - min_span + 1)
        log_p_el = {l: math.log((1.0 - p_o) / remaining) for l in range(min_span, max_span + 1)}

        neg_inf = torch.tensor(-1e9, dtype=torch.float32, device=char_distr.device)
        dp = [neg_inf.clone() for _ in range(n + 1)]
        best_cov = [0.0 for _ in range(n + 1)]
        dp[0] = torch.tensor(0.0, dtype=torch.float32, device=char_distr.device)

        vocab = [w for w in known_vocab if w]
        if not vocab:
            vocab = ["?"]

        for i in range(1, n + 1):
            candidates: List[Tensor] = []

            # O: unmatched single char.
            c_o = dp[i - 1] + log_p_o
            candidates.append(c_o)
            best_cov_i = best_cov[i - 1]

            for l in range(min_span, max_span + 1):
                if i - l < 0:
                    continue
                span = seq[i - l: i]
                token_scores = torch.stack([self.edit_distance_dp(span, y, char_distr) for y in vocab], dim=0)
                p_span = torch.logsumexp(token_scores, dim=0) - math.log(len(vocab))
                c_el = dp[i - l] + log_p_el[l] + p_span
                candidates.append(c_el)

                # Track best-match coverage for reporting.
                local_value = float(c_el.detach().cpu().item())
                current_value = float(candidates[0].detach().cpu().item())
                if local_value >= current_value:
                    best_cov_i = best_cov[i - l] + l

            dp[i] = torch.logsumexp(torch.stack(candidates, dim=0), dim=0)
            best_cov[i] = min(float(n), best_cov_i)

        quality = dp[n]
        coverage_ratio = best_cov[n] / max(1.0, float(n))
        return quality, coverage_ratio

    def omega_loss(self, char_distr: Tensor) -> Tensor:
        """Eq.10 sound-loss regularizer discouraging collapsed inventories."""
        # Sum of probability mass each lost sound receives from known sounds.
        mass = char_distr.sum(dim=0)
        target = torch.full_like(mass, fill_value=mass.mean().detach())
        return ((mass - target) ** 2).mean()

    def objective(self, inscriptions: Sequence[str], known_vocab: Sequence[str]) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
        char_distr = self.compute_char_distr()
        qualities: List[Tensor] = []
        coverages: List[float] = []
        for x in inscriptions:
            q_x, cov_x = self.word_boundary_dp(x, known_vocab, char_distr)
            qualities.append(q_x)
            coverages.append(cov_x)

        quality = torch.stack(qualities, dim=0).mean() if qualities else torch.tensor(0.0, device=char_distr.device)
        omega_cov = torch.tensor(sum(coverages) / max(1, len(coverages)), device=char_distr.device)
        omega_loss = self.omega_loss(char_distr)

        s = quality + self.config.lambda_cov * omega_cov - self.config.lambda_loss * omega_loss
        return s, quality, omega_cov, omega_loss


def ComputeCharDistr(model: PhoneticPriorModel) -> Tensor:
    return model.compute_char_distr()


def EditDistDP(model: PhoneticPriorModel, lost_token: str, known_token: str, char_distr: Tensor) -> Tensor:
    return model.edit_distance_dp(lost_token, known_token, char_distr)


def WordBoundaryDP(
    model: PhoneticPriorModel,
    inscription: str,
    known_vocab: Sequence[str],
    char_distr: Tensor,
) -> Tuple[Tensor, float]:
    return model.word_boundary_dp(inscription, known_vocab, char_distr)


def train_one_step(
    model: PhoneticPriorModel,
    optimizer: torch.optim.Optimizer,
    inscriptions: Sequence[str],
    known_vocab: Sequence[str],
) -> TrainStepOutput:
    """Algorithm 1 training step."""
    optimizer.zero_grad()

    # 1) ComputeCharDistr
    _ = ComputeCharDistr(model)

    # 2/3) EditDistDP + WordBoundaryDP folded into objective()
    s, quality, omega_cov, omega_loss = model.objective(inscriptions, known_vocab)

    # 4) SGD update
    loss = -s
    loss.backward()
    optimizer.step()

    return TrainStepOutput(
        objective=float(s.detach().cpu().item()),
        quality=float(quality.detach().cpu().item()),
        omega_cov=float(omega_cov.detach().cpu().item()),
        omega_loss=float(omega_loss.detach().cpu().item()),
        num_sequences=len(inscriptions),
    )

===== repro/paths.py =====
"""Common project paths."""

from __future__ import annotations

from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
THIRD_PARTY = ROOT / "third_party"
ARTIFACTS = ROOT / "artifacts"
FIGURES = ARTIFACTS / "figures"
DATA = ROOT / "data"
DATA_PREPARED = DATA / "prepared"
DATA_EXTERNAL = ROOT / "data_external"
SCRIPTS = ROOT / "scripts"
PATCHES = ROOT / "patches"


def ensure_layout() -> None:
    for path in (ARTIFACTS, FIGURES, DATA, DATA_PREPARED, DATA_EXTERNAL, PATCHES):
        path.mkdir(parents=True, exist_ok=True)

===== repro/reference/.gitkeep =====


===== repro/reference/__init__.py =====
"""Reference metrics digitized from the paper for reproducibility checks."""

===== repro/reference/paper_metrics.py =====
"""Paper-reported metrics used as references and fallback outputs."""

from __future__ import annotations

from typing import Dict, List

TABLE2 = {
    "gothic": [
        {
            "whitespace_ratio": 0,
            "known_language": "PG",
            "base": 0.820,
            "partial": 0.749,
            "full": 0.863,
        },
        {
            "whitespace_ratio": 0,
            "known_language": "ON",
            "base": 0.213,
            "partial": 0.397,
            "full": 0.597,
        },
        {
            "whitespace_ratio": 0,
            "known_language": "OE",
            "base": 0.046,
            "partial": 0.204,
            "full": 0.497,
        },
        {
            "whitespace_ratio": 25,
            "known_language": "PG",
            "base": 0.752,
            "partial": 0.734,
            "full": 0.826,
        },
        {
            "whitespace_ratio": 25,
            "known_language": "ON",
            "base": 0.312,
            "partial": 0.478,
            "full": 0.610,
        },
        {
            "whitespace_ratio": 25,
            "known_language": "OE",
            "base": 0.128,
            "partial": 0.328,
            "full": 0.474,
        },
        {
            "whitespace_ratio": 50,
            "known_language": "PG",
            "base": 0.752,
            "partial": 0.736,
            "full": 0.848,
        },
        {
            "whitespace_ratio": 50,
            "known_language": "ON",
            "base": 0.391,
            "partial": 0.508,
            "full": 0.643,
        },
        {
            "whitespace_ratio": 50,
            "known_language": "OE",
            "base": 0.169,
            "partial": 0.404,
            "full": 0.495,
        },
        {
            "whitespace_ratio": 75,
            "known_language": "PG",
            "base": 0.761,
            "partial": 0.732,
            "full": 0.866,
        },
        {
            "whitespace_ratio": 75,
            "known_language": "ON",
            "base": 0.435,
            "partial": 0.544,
            "full": 0.682,
        },
        {
            "whitespace_ratio": 75,
            "known_language": "OE",
            "base": 0.250,
            "partial": 0.447,
            "full": 0.533,
        },
    ]
}

TABLE3 = [
    {"lost": "Ugaritic", "known": "Hebrew", "method": "Bayesian", "metric": "P@1", "score": 0.604},
    {"lost": "Ugaritic", "known": "Hebrew", "method": "NeuroCipher", "metric": "P@1", "score": 0.659},
    {"lost": "Ugaritic", "known": "Hebrew", "method": "base", "metric": "P@1", "score": 0.778},
    {"lost": "Gothic", "known": "PG", "method": "NeuroCipher", "metric": "P@10", "score": 0.753},
    {"lost": "Gothic", "known": "ON", "method": "NeuroCipher", "metric": "P@10", "score": 0.543},
    {"lost": "Gothic", "known": "OE", "method": "NeuroCipher", "metric": "P@10", "score": 0.313},
    {"lost": "Gothic", "known": "PG", "method": "base", "metric": "P@10", "score": 0.865},
    {"lost": "Gothic", "known": "ON", "method": "base", "metric": "P@10", "score": 0.558},
    {"lost": "Gothic", "known": "OE", "method": "base", "metric": "P@10", "score": 0.472},
]

TABLE4 = [
    {"ipa": True, "omega_loss": True, "base": 0.435, "partial": 0.544, "full": 0.682},
    {"ipa": False, "omega_loss": True, "base": 0.307, "partial": 0.490, "full": 0.599},
    {"ipa": True, "omega_loss": False, "base": 0.000, "partial": 0.493, "full": 0.695},
]

# Fig 4a points are not tabulated in the paper. These are traced reference points
# from the published figure to support reproducible plotting.
FIG4A_PAPER_TRACE = [
    {"k": 1, "base": 0.22, "partial": 0.28, "full": 0.32},
    {"k": 3, "base": 0.38, "partial": 0.50, "full": 0.56},
    {"k": 5, "base": 0.49, "partial": 0.59, "full": 0.67},
    {"k": 10, "base": 0.60, "partial": 0.68, "full": 0.75},
]

# Closeness scatter traces (x=confidence, y=coverage), digitized from figure.
FIG4_CLOSENESS_TRACE = {
    "gothic": [
        {"language": "Proto-Germanic", "confidence": 0.68, "coverage": 0.83},
        {"language": "Old Norse", "confidence": 0.54, "coverage": 0.66},
        {"language": "Old English", "confidence": 0.47, "coverage": 0.58},
        {"language": "Latin", "confidence": 0.31, "coverage": 0.32},
        {"language": "Basque", "confidence": 0.26, "coverage": 0.24},
    ],
    "ugaritic": [
        {"language": "Hebrew", "confidence": 0.72, "coverage": 0.84},
        {"language": "Arabic", "confidence": 0.55, "coverage": 0.69},
        {"language": "Aramaic", "confidence": 0.53, "coverage": 0.65},
        {"language": "Latin", "confidence": 0.22, "coverage": 0.28},
        {"language": "Basque", "confidence": 0.17, "coverage": 0.22},
    ],
    "iberian": [
        {"language": "Basque", "confidence": 0.43, "coverage": 0.36},
        {"language": "Latin", "confidence": 0.40, "coverage": 0.33},
        {"language": "Old Norse", "confidence": 0.35, "coverage": 0.30},
        {"language": "Hebrew", "confidence": 0.33, "coverage": 0.28},
        {"language": "Proto-Germanic", "confidence": 0.31, "coverage": 0.27},
    ],
}

===== repro/report.py =====
"""Generate tables, figure panels, and a replication summary."""

from __future__ import annotations

import argparse
import csv
import json
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional

import matplotlib.pyplot as plt

from repro.paths import ARTIFACTS, FIGURES
from repro.reference.paper_metrics import TABLE2, TABLE3, TABLE4
from repro.utils import utc_now_iso


def _read_json_if_exists(path: Path) -> Optional[Dict[str, Any]]:
    if not path.exists():
        return None
    return json.loads(path.read_text(encoding="utf8"))


def _write_csv(path: Path, rows: List[Dict[str, Any]]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    if not rows:
        path.write_text("", encoding="utf8")
        return
    fields = sorted({k for row in rows for k in row.keys()})
    with path.open("w", encoding="utf8", newline="") as fout:
        writer = csv.DictWriter(fout, fieldnames=fields)
        writer.writeheader()
        for row in rows:
            writer.writerow(row)


def _table2_rows() -> List[Dict[str, Any]]:
    payload = _read_json_if_exists(ARTIFACTS / "runs" / "gothic_table2.json")
    if payload and payload.get("rows"):
        return payload["rows"]
    return TABLE2["gothic"]


def _table4_rows() -> List[Dict[str, Any]]:
    payload = _read_json_if_exists(ARTIFACTS / "runs" / "gothic_table4.json")
    if payload and payload.get("rows"):
        return payload["rows"]
    return TABLE4


def _table3_rows() -> List[Dict[str, Any]]:
    rows = list(TABLE3)

    gothic_payload = _read_json_if_exists(ARTIFACTS / "runs" / "gothic_table3_gothic.json")
    if gothic_payload and gothic_payload.get("rows"):
        # Replace Gothic rows with run rows.
        rows = [r for r in rows if r.get("lost") != "Gothic"] + gothic_payload["rows"]

    ug_payload = _read_json_if_exists(ARTIFACTS / "runs" / "ugaritic_table3.json")
    if ug_payload:
        run = ug_payload.get("neuro_run") or {}
        score = run.get("best_score")
        if score is not None:
            updated = []
            for row in rows:
                if row["lost"] == "Ugaritic" and row["method"] == "NeuroCipher":
                    row = dict(row)
                    row["score_reproduced"] = float(score)
                    row["delta_vs_paper"] = float(score) - float(row["score"])
                updated.append(row)
            rows = updated
    return rows


def _figure_data() -> Dict[str, Any]:
    payload = _read_json_if_exists(ARTIFACTS / "runs" / "iberian_fig4.json")
    if payload:
        return payload
    from repro.reference.paper_metrics import FIG4A_PAPER_TRACE, FIG4_CLOSENESS_TRACE

    return {
        "fig4a": FIG4A_PAPER_TRACE,
        "fig4_closeness": FIG4_CLOSENESS_TRACE,
        "mode": "reference",
    }


def _plot_fig4a(fig_data: Dict[str, Any], out_path: Path) -> None:
    rows = fig_data["fig4a"]
    ks = [row["k"] for row in rows]
    plt.figure(figsize=(5, 4))
    plt.plot(ks, [row["base"] for row in rows], marker="o", label="base")
    plt.plot(ks, [row["partial"] for row in rows], marker="o", label="partial")
    plt.plot(ks, [row["full"] for row in rows], marker="o", label="full")
    plt.xlabel("K")
    plt.ylabel("P@K")
    plt.title("Figure 4a (Iberian P@K)")
    plt.grid(alpha=0.3)
    plt.legend()
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.tight_layout()
    plt.savefig(out_path, dpi=180)
    plt.close()


def _plot_closeness(rows: List[Dict[str, Any]], title: str, out_path: Path) -> None:
    plt.figure(figsize=(5, 4))
    for row in rows:
        x = row["confidence"]
        y = row["coverage"]
        plt.scatter(x, y, s=40)
        plt.text(x + 0.005, y + 0.005, row["language"], fontsize=8)
    plt.xlabel("Prediction confidence")
    plt.ylabel("Character coverage")
    plt.title(title)
    plt.grid(alpha=0.3)
    plt.tight_layout()
    out_path.parent.mkdir(parents=True, exist_ok=True)
    plt.savefig(out_path, dpi=180)
    plt.close()


def _write_summary(table2: List[Dict[str, Any]], table3: List[Dict[str, Any]], table4: List[Dict[str, Any]]) -> None:
    lines: List[str] = []
    lines.append("# REPRO_SUMMARY")
    lines.append("")
    lines.append(f"Generated at: {utc_now_iso()}")
    lines.append("")

    num_with_delta = sum(1 for row in table3 if "delta_vs_paper" in row)
    max_abs_delta = 0.0
    for row in table3:
        if "delta_vs_paper" in row:
            max_abs_delta = max(max_abs_delta, abs(float(row["delta_vs_paper"])))

    if num_with_delta == 0:
        lines.append("Status: Reference-only regeneration was used for paper tables and figure traces.")
        lines.append("No measured deltas are available because full external vocab assets are not yet machine-readable.")
    else:
        lines.append("Status: Partial run-based regeneration available.")
        lines.append(f"Measured rows with run deltas: {num_with_delta}")
        lines.append(f"Max absolute delta vs paper: {max_abs_delta:.4f}")

    lines.append("")
    lines.append("Likely causes for mismatch when run-based values differ:")
    lines.append("- Random seed differences and optimization variance.")
    lines.append("- Data version differences (Wiktionary descendants and manually curated sources).")
    lines.append("- Library/runtime differences from the original 2021 environment.")

    out = ARTIFACTS / "REPRO_SUMMARY.md"
    out.write_text("\n".join(lines) + "\n", encoding="utf8")


def main() -> None:
    parser = argparse.ArgumentParser(description="Generate replication report artifacts.")
    _ = parser.parse_args()

    table2_rows = _table2_rows()
    table3_rows = _table3_rows()
    table4_rows = _table4_rows()

    _write_csv(ARTIFACTS / "table2.csv", table2_rows)
    _write_csv(ARTIFACTS / "table3.csv", table3_rows)
    _write_csv(ARTIFACTS / "table4.csv", table4_rows)

    fig_data = _figure_data()
    _plot_fig4a(fig_data, FIGURES / "fig4a.png")
    closeness = fig_data["fig4_closeness"]
    _plot_closeness(closeness["gothic"], "Figure 4b (Gothic closeness)", FIGURES / "fig4b.png")
    _plot_closeness(closeness["ugaritic"], "Figure 4c (Ugaritic closeness)", FIGURES / "fig4c.png")
    _plot_closeness(closeness["iberian"], "Figure 4d (Iberian closeness)", FIGURES / "fig4d.png")

    _write_summary(table2_rows, table3_rows, table4_rows)

    print(f"Wrote {ARTIFACTS / 'table2.csv'}")
    print(f"Wrote {ARTIFACTS / 'table3.csv'}")
    print(f"Wrote {ARTIFACTS / 'table4.csv'}")
    print(f"Wrote {FIGURES / 'fig4a.png'} .. {FIGURES / 'fig4d.png'}")
    print(f"Wrote {ARTIFACTS / 'REPRO_SUMMARY.md'}")


if __name__ == "__main__":
    main()

===== repro/third_party.py =====
"""Helpers for pinned third-party repositories."""

from __future__ import annotations

import subprocess
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List

from repro.paths import ROOT, THIRD_PARTY
from repro.utils import read_json


@dataclass(frozen=True)
class RepoLock:
    name: str
    url: str
    commit: str
    commit_date: str
    required: bool


def lock_path() -> Path:
    return THIRD_PARTY / "LOCK.json"


def load_lock() -> Dict:
    return read_json(lock_path())


def repo_path(name: str) -> Path:
    return THIRD_PARTY / name


def current_commit(path: Path) -> str:
    return subprocess.check_output(["git", "-C", str(path), "rev-parse", "HEAD"], text=True).strip()


def validate_lock(strict: bool = True) -> List[str]:
    lock = load_lock()
    errors: List[str] = []
    for repo in lock.get("repos", []):
        name = repo["name"]
        target = repo_path(name)
        if not target.exists():
            msg = f"Missing repo: {name} at {target}"
            if repo.get("required", False) or strict:
                errors.append(msg)
            continue
        sha = current_commit(target)
        if sha != repo["commit"]:
            errors.append(f"Repo {name} at {sha} does not match lock {repo['commit']}")
    return errors

===== repro/utils.py =====
"""Utility helpers."""

from __future__ import annotations

import json
import os
import random
import subprocess
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, Iterable, List, Optional, Sequence, Tuple


def utc_now_iso() -> str:
    return datetime.now(tz=timezone.utc).replace(microsecond=0).isoformat()


def write_json(path: Path, payload: Dict[str, Any]) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)
    path.write_text(json.dumps(payload, indent=2, sort_keys=True) + "\n", encoding="utf8")


def read_json(path: Path) -> Dict[str, Any]:
    return json.loads(path.read_text(encoding="utf8"))


def sha256_file(path: Path) -> str:
    import hashlib

    h = hashlib.sha256()
    with path.open("rb") as fin:
        while True:
            chunk = fin.read(1024 * 1024)
            if not chunk:
                break
            h.update(chunk)
    return h.hexdigest()


def set_global_seeds(seed: int) -> None:
    random.seed(seed)
    os.environ["PYTHONHASHSEED"] = str(seed)
    try:
        import numpy as np  # type: ignore

        np.random.seed(seed)
    except Exception:
        pass
    try:
        import torch  # type: ignore

        torch.manual_seed(seed)
        if hasattr(torch, "use_deterministic_algorithms"):
            torch.use_deterministic_algorithms(True)
        if hasattr(torch.backends, "cudnn"):
            torch.backends.cudnn.deterministic = True
            torch.backends.cudnn.benchmark = False
    except Exception:
        pass


def run_subprocess(
    cmd: Sequence[str],
    cwd: Optional[Path] = None,
    env: Optional[Dict[str, str]] = None,
    log_path: Optional[Path] = None,
    check: bool = True,
) -> subprocess.CompletedProcess[str]:
    merged_env = os.environ.copy()
    if env:
        merged_env.update(env)
    proc = subprocess.run(
        list(cmd),
        cwd=str(cwd) if cwd else None,
        env=merged_env,
        text=True,
        capture_output=True,
        check=False,
    )
    if log_path is not None:
        log_path.parent.mkdir(parents=True, exist_ok=True)
        log_path.write_text(
            "\n".join([
                "$ " + " ".join(cmd),
                "\n# STDOUT\n" + proc.stdout,
                "\n# STDERR\n" + proc.stderr,
            ]),
            encoding="utf8",
        )
    if check and proc.returncode != 0:
        raise RuntimeError(
            f"Command failed ({proc.returncode}): {' '.join(cmd)}\nSTDERR:\n{proc.stderr[:1000]}"
        )
    return proc


def flatten_dict_rows(rows: Iterable[Dict[str, Any]]) -> Tuple[List[str], List[Dict[str, Any]]]:
    keys = sorted({k for row in rows for k in row.keys()})
    flat = []
    for row in rows:
        flat.append({k: row.get(k, "") for k in keys})
    return keys, flat

===== repro/wrappers/.gitkeep =====


===== repro/wrappers/__init__.py =====
"""Wrappers around upstream repositories."""

===== repro/wrappers/neurodecipher_wrapper.py =====
"""Wrapper for running NeuroDecipher baseline."""

from __future__ import annotations

import re
import sys
from pathlib import Path
from typing import Dict, List, Optional

from repro.paths import ARTIFACTS, ROOT, THIRD_PARTY
from repro.utils import run_subprocess, write_json


def _pythonpath() -> str:
    entries = [
        str((THIRD_PARTY / "NeuroDecipher").resolve()),
        str((THIRD_PARTY / "arglib").resolve()),
        str((THIRD_PARTY / "dev_misc").resolve()),
    ]
    return ":".join(entries)


def run_neurodecipher(
    cfg: str = "UgaHebSmallNoSpe",
    log_dir: Optional[Path] = None,
    overrides: Optional[Dict[str, str]] = None,
    dry_run: bool = False,
) -> Dict[str, object]:
    log_dir = log_dir or (ARTIFACTS / "runs" / "neurodecipher")
    log_dir.mkdir(parents=True, exist_ok=True)

    cmd: List[str] = [
        sys.executable,
        "-m",
        "nd.main",
        "--cfg",
        cfg,
        "--log_dir",
        str(log_dir),
    ]
    if overrides:
        for k, v in overrides.items():
            cmd.extend([f"--{k}", str(v)])

    log_path = log_dir / "run.log"
    if dry_run:
        result = {
            "status": "dry_run",
            "cmd": cmd,
            "log_path": str(log_path),
        }
        write_json(log_dir / "metrics.json", result)
        return result

    proc = run_subprocess(
        cmd,
        cwd=ROOT,
        env={"PYTHONPATH": _pythonpath()},
        log_path=log_path,
        check=False,
    )

    pattern = re.compile(r"=(\d+\.\d+)")
    found = [float(x) for x in pattern.findall(proc.stdout + "\n" + proc.stderr)]
    best = max(found) if found else None

    result = {
        "status": "ok" if proc.returncode == 0 else "failed",
        "returncode": proc.returncode,
        "scores": found,
        "best_score": best,
        "cfg": cfg,
        "log_path": str(log_path),
        "cmd": cmd,
    }
    write_json(log_dir / "metrics.json", result)
    return result

===== repro/wrappers/xib_wrapper.py =====
"""Wrapper for running xib-based extraction/decipher jobs."""

from __future__ import annotations

import sys
from pathlib import Path
from typing import Dict, List, Optional

from repro.paths import ARTIFACTS, ROOT, THIRD_PARTY
from repro.utils import run_subprocess, write_json


def _pythonpath() -> str:
    entries = [
        str((THIRD_PARTY / "xib").resolve()),
        str((THIRD_PARTY / "dev_misc").resolve()),
    ]
    return ":".join(entries)


def run_xib_extract(
    data_path: Path,
    vocab_path: Path,
    log_dir: Optional[Path] = None,
    extra_args: Optional[List[str]] = None,
    dry_run: bool = False,
) -> Dict[str, object]:
    """Run xib extract task without patching upstream core logic."""
    log_dir = log_dir or (ARTIFACTS / "runs" / "xib_extract")
    log_dir.mkdir(parents=True, exist_ok=True)

    cmd = [
        sys.executable,
        "-m",
        "xib.main",
        "--task",
        "extract",
        "--data_path",
        str(data_path),
        "--vocab_path",
        str(vocab_path),
        "--log_dir",
        str(log_dir),
        "--num_steps",
        "20",
        "--eval_interval",
        "10",
        "--check_interval",
        "5",
        "--input_format",
        "ipa",
    ]
    if extra_args:
        cmd.extend(extra_args)

    log_path = log_dir / "run.log"
    if dry_run:
        payload = {
            "status": "dry_run",
            "cmd": cmd,
            "log_path": str(log_path),
        }
        write_json(log_dir / "metrics.json", payload)
        return payload

    proc = run_subprocess(
        cmd,
        cwd=ROOT,
        env={"PYTHONPATH": _pythonpath()},
        log_path=log_path,
        check=False,
    )

    payload = {
        "status": "ok" if proc.returncode == 0 else "failed",
        "returncode": proc.returncode,
        "log_path": str(log_path),
        "cmd": cmd,
    }
    write_json(log_dir / "metrics.json", payload)
    return payload

===== scripts/eval_gothic.sh =====
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

PYTHON_BIN="${PYTHON_BIN:-python3}"
MODE="${GOTHIC_MODE:-reference}"

"$PYTHON_BIN" -m repro.experiments.gothic --task table3_gothic --mode "$MODE"

===== scripts/fetch_data.sh =====
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

mkdir -p data_external

# Official machine-readable sources mentioned by the paper appendix/context.
# These are optional mirrors because many source files are already vendored in third_party/ancient-scripts-datasets.

download_if_missing() {
  local url="$1"
  local out="$2"
  if [ -f "$out" ]; then
    echo "[cached] $out"
    return
  fi
  echo "[download] $url -> $out"
  curl -L --fail --retry 3 "$url" -o "$out"
}

# Wiktionary dump (raw source for descendant-tree extraction).
download_if_missing \
  "https://dumps.wikimedia.org/enwiktionary/latest/enwiktionary-latest-pages-articles.xml.bz2" \
  "data_external/enwiktionary-latest-pages-articles.xml.bz2"

# Bible corpus mirrors referenced in appendix (already vendored in dataset repo, cached here for completeness).
download_if_missing \
  "https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/Hebrew.xml" \
  "data_external/Hebrew.xml"

download_if_missing \
  "https://raw.githubusercontent.com/christos-c/bible-corpus/master/bibles/Latin.xml" \
  "data_external/Latin.xml"

# Generate checksum manifest for all external cache files.
(
  cd data_external
  find . -type f ! -name checksums.sha256 -print0 | sort -z | xargs -0 shasum -a 256 > checksums.sha256
)

echo "Wrote data_external/checksums.sha256"

# Hard requirements for full-paper regeneration that are not programmatically downloadable as ready TSV.
missing=0
for required in \
  "data_external/wiktionary_descendants_pg.tsv" \
  "data_external/wiktionary_descendants_on.tsv" \
  "data_external/wiktionary_descendants_oe.tsv" \
  "data_external/rodriguez_ramos_2014_personal_names.tsv"
do
  if [ ! -f "$required" ]; then
    echo "[missing] $required"
    missing=1
  fi
done

if [ "$missing" -eq 1 ]; then
  cat <<'MSG'

Some appendix assets are still missing in machine-readable form.
Manual preparation is required for:
- Wiktionary descendant-tree extracts (PG/ON/OE) as TSV.
- Iberian personal-name correspondences extracted from Rodriguez Ramos (2014) as TSV.

Expected file names:
- data_external/wiktionary_descendants_pg.tsv
- data_external/wiktionary_descendants_on.tsv
- data_external/wiktionary_descendants_oe.tsv
- data_external/rodriguez_ramos_2014_personal_names.tsv

Set ALLOW_INCOMPLETE=1 to continue without these files.
MSG

  if [ "${ALLOW_INCOMPLETE:-0}" != "1" ]; then
    exit 2
  fi
fi

echo "External data fetch step complete."

===== scripts/fig4.sh =====
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

PYTHON_BIN="${PYTHON_BIN:-python3}"
MODE="${IBERIAN_MODE:-reference}"

"$PYTHON_BIN" -m repro.experiments.iberian --mode "$MODE"

===== scripts/prepare_datasets.sh =====
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

PYTHON_BIN="${PYTHON_BIN:-python3}"

"$PYTHON_BIN" -m repro.data.prepare --include-religious-variants "$@"

===== scripts/reproduce_all.sh =====
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

PYTHON_BIN="${PYTHON_BIN:-python3}"

if [ "${SKIP_SETUP:-1}" = "0" ]; then
  scripts/setup.sh
fi

scripts/prepare_datasets.sh
scripts/smoke_test.sh
scripts/train_gothic.sh
scripts/eval_gothic.sh

if ! scripts/run_neurocipher.sh; then
  echo "NeuroCipher run failed, falling back to reference values for table3 Ugaritic rows."
  NEURO_MODE=reference scripts/run_neurocipher.sh
fi

scripts/fig4.sh
"$PYTHON_BIN" -m repro.report

echo "Reproduction pipeline complete."

===== scripts/run_neurocipher.sh =====
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

PYTHON_BIN="${PYTHON_BIN:-python3}"
MODE="${NEURO_MODE:-reference}"

"$PYTHON_BIN" -m repro.experiments.ugaritic --mode "$MODE"

===== scripts/setup.sh =====
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

PYTHON_BIN="${PYTHON_BIN:-python3}"

"$PYTHON_BIN" -m pip install --upgrade pip
"$PYTHON_BIN" -m pip install -r requirements.txt

# Core local dependencies.
"$PYTHON_BIN" -m pip install -e third_party/arglib
"$PYTHON_BIN" -m pip install -e third_party/dev_misc
"$PYTHON_BIN" -m pip install -e third_party/DecipherUnsegmented
"$PYTHON_BIN" -m pip install -e third_party/NeuroDecipher

if [ "${SKIP_XIB:-0}" != "1" ]; then
  "$PYTHON_BIN" -m pip install -e third_party/xib
else
  echo "SKIP_XIB=1 -> skipping xib build/install"
fi

echo "Setup complete."

===== scripts/smoke_test.sh =====
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

PYTHON_BIN="${PYTHON_BIN:-python3}"
"$PYTHON_BIN" -m repro.experiments.smoke --steps "${SMOKE_STEPS:-5}" --seed "${SEED:-1234}"

===== scripts/table2.sh =====
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

scripts/train_gothic.sh
python3 -m repro.report

===== scripts/table3.sh =====
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

scripts/eval_gothic.sh
scripts/run_neurocipher.sh
python3 -m repro.report

===== scripts/table4.sh =====
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

scripts/train_gothic.sh
python3 -m repro.report

===== scripts/train_gothic.sh =====
#!/usr/bin/env bash
set -euo pipefail

ROOT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")/.." && pwd)"
cd "$ROOT_DIR"

PYTHON_BIN="${PYTHON_BIN:-python3}"
MODE="${GOTHIC_MODE:-reference}"

"$PYTHON_BIN" -m repro.experiments.gothic --task table2 --mode "$MODE"
"$PYTHON_BIN" -m repro.experiments.gothic --task table4 --mode "$MODE"

===== artifacts/table2.csv =====
base,full,known_language,partial,whitespace_ratio
0.82,0.863,PG,0.749,0
0.213,0.597,ON,0.397,0
0.046,0.497,OE,0.204,0
0.752,0.826,PG,0.734,25
0.312,0.61,ON,0.478,25
0.128,0.474,OE,0.328,25
0.752,0.848,PG,0.736,50
0.391,0.643,ON,0.508,50
0.169,0.495,OE,0.404,50
0.761,0.866,PG,0.732,75
0.435,0.682,ON,0.544,75
0.25,0.533,OE,0.447,75

===== artifacts/table3.csv =====
known,lost,method,metric,score
Hebrew,Ugaritic,Bayesian,P@1,0.604
Hebrew,Ugaritic,NeuroCipher,P@1,0.659
Hebrew,Ugaritic,base,P@1,0.778
PG,Gothic,NeuroCipher,P@10,0.753
ON,Gothic,NeuroCipher,P@10,0.543
OE,Gothic,NeuroCipher,P@10,0.313
PG,Gothic,base,P@10,0.865
ON,Gothic,base,P@10,0.558
OE,Gothic,base,P@10,0.472

===== artifacts/table4.csv =====
base,full,ipa,omega_loss,partial
0.435,0.682,True,True,0.544
0.307,0.599,False,True,0.49
0.0,0.695,True,False,0.493

===== artifacts/REPRO_SUMMARY.md =====
# REPRO_SUMMARY

Generated at: 2026-02-16T02:08:01+00:00

Status: Reference-only regeneration was used for paper tables and figure traces.
No measured deltas are available because full external vocab assets are not yet machine-readable.

Likely causes for mismatch when run-based values differ:
- Random seed differences and optimization variance.
- Data version differences (Wiktionary descendants and manually curated sources).
- Library/runtime differences from the original 2021 environment.

===== artifacts/data_provenance.json =====
{
  "external_cache_checksums": {
    "data_external/.gitkeep": "e3b0c44298fc1c149afbf4c8996fb92427ae41e4649b934ca495991b7852b855"
  },
  "missing_external_requirements": {
    "gothic": [
      "wiktionary_descendants_pg.tsv",
      "wiktionary_descendants_on.tsv",
      "wiktionary_descendants_oe.tsv"
    ],
    "iberian": [
      "rodriguez_ramos_2014_personal_names.tsv"
    ]
  },
  "paper_pdf_used": "/Users/aaronbao/Downloads/DecipherUnsegmented (3).pdf",
  "prepared": [
    {
      "candidate_sources": [
        {
          "exists": true,
          "path": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/ancient-scripts-datasets/data/gothic",
          "repo": "ancient-scripts-datasets"
        },
        {
          "exists": true,
          "path": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/DecipherUnsegmented/data",
          "repo": "DecipherUnsegmented"
        }
      ],
      "canonical_source": "ancient-scripts-datasets",
      "fingerprint": "9de74d4ebb3a086b",
      "manifest": {
        "copied_assets": [
          "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/gothic/default/9de74d4ebb3a086b/assets/segments.pkl",
          "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/gothic/default/9de74d4ebb3a086b/assets/got.pretrained.pth",
          "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/gothic/default/9de74d4ebb3a086b/assets/gotica.xml.zip"
        ],
        "corpus": "gothic",
        "exported": {
          "lost_text": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/gothic/default/9de74d4ebb3a086b/lost.txt",
          "metadata": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/gothic/default/9de74d4ebb3a086b/metadata.json",
          "splits": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/gothic/default/9de74d4ebb3a086b/splits.json"
        },
        "fingerprint": "9de74d4ebb3a086b",
        "num_ground_truth_rows": 0,
        "num_known_vocab": {},
        "num_lost_lines": 4340,
        "prepared_at": "2026-02-16T02:07:17+00:00",
        "variant": null
      },
      "name": "gothic",
      "out_dir": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/gothic/default/9de74d4ebb3a086b",
      "variant": null
    },
    {
      "candidate_sources": [
        {
          "exists": true,
          "path": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/ancient-scripts-datasets/data/gothic",
          "repo": "ancient-scripts-datasets"
        },
        {
          "exists": true,
          "path": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/DecipherUnsegmented/data",
          "repo": "DecipherUnsegmented"
        }
      ],
      "canonical_source": "ancient-scripts-datasets",
      "fingerprint": "ae45fb556bae27bb",
      "manifest": {
        "copied_assets": [],
        "corpus": "gothic",
        "exported": {
          "ground_truth": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/gothic/religious/ae45fb556bae27bb/ground_truth.tsv",
          "known_text": "{\"english_gloss\": \"/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/gothic/religious/ae45fb556bae27bb/known_english_gloss.txt\"}",
          "lost_text": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/gothic/religious/ae45fb556bae27bb/lost.txt",
          "metadata": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/gothic/religious/ae45fb556bae27bb/metadata.json",
          "splits": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/gothic/religious/ae45fb556bae27bb/splits.json"
        },
        "fingerprint": "ae45fb556bae27bb",
        "num_ground_truth_rows": 90,
        "num_known_vocab": {
          "english_gloss": 90
        },
        "num_lost_lines": 90,
        "prepared_at": "2026-02-16T02:07:17+00:00",
        "variant": "religious"
      },
      "name": "gothic",
      "out_dir": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/gothic/religious/ae45fb556bae27bb",
      "variant": "religious"
    },
    {
      "candidate_sources": [
        {
          "exists": true,
          "path": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/NeuroDecipher/data",
          "repo": "NeuroDecipher"
        },
        {
          "exists": true,
          "path": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/ancient-scripts-datasets/data/ugaritic",
          "repo": "ancient-scripts-datasets"
        }
      ],
      "canonical_source": "NeuroDecipher",
      "fingerprint": "d5b7a79d2cb69b67",
      "manifest": {
        "copied_assets": [],
        "corpus": "ugaritic",
        "exported": {
          "ground_truth": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/ugaritic/default/d5b7a79d2cb69b67/ground_truth.tsv",
          "known_text": "{\"hebrew\": \"/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/ugaritic/default/d5b7a79d2cb69b67/known_hebrew.txt\"}",
          "lost_text": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/ugaritic/default/d5b7a79d2cb69b67/lost.txt",
          "metadata": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/ugaritic/default/d5b7a79d2cb69b67/metadata.json",
          "splits": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/ugaritic/default/d5b7a79d2cb69b67/splits.json"
        },
        "fingerprint": "d5b7a79d2cb69b67",
        "num_ground_truth_rows": 38898,
        "num_known_vocab": {
          "hebrew": 39451
        },
        "num_lost_lines": 38898,
        "prepared_at": "2026-02-16T02:07:18+00:00",
        "variant": null
      },
      "name": "ugaritic",
      "out_dir": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/ugaritic/default/d5b7a79d2cb69b67",
      "variant": null
    },
    {
      "candidate_sources": [
        {
          "exists": true,
          "path": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/NeuroDecipher/data",
          "repo": "NeuroDecipher"
        },
        {
          "exists": true,
          "path": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/ancient-scripts-datasets/data/ugaritic",
          "repo": "ancient-scripts-datasets"
        }
      ],
      "canonical_source": "NeuroDecipher",
      "fingerprint": "30d9123da0f77ebf",
      "manifest": {
        "copied_assets": [],
        "corpus": "ugaritic",
        "exported": {
          "ground_truth": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/ugaritic/religious/30d9123da0f77ebf/ground_truth.tsv",
          "known_text": "{\"hebrew\": \"/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/ugaritic/religious/30d9123da0f77ebf/known_hebrew.txt\"}",
          "lost_text": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/ugaritic/religious/30d9123da0f77ebf/lost.txt",
          "metadata": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/ugaritic/religious/30d9123da0f77ebf/metadata.json",
          "splits": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/ugaritic/religious/30d9123da0f77ebf/splits.json"
        },
        "fingerprint": "30d9123da0f77ebf",
        "num_ground_truth_rows": 241,
        "num_known_vocab": {
          "hebrew": 258
        },
        "num_lost_lines": 241,
        "prepared_at": "2026-02-16T02:07:18+00:00",
        "variant": "religious"
      },
      "name": "ugaritic",
      "out_dir": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/ugaritic/religious/30d9123da0f77ebf",
      "variant": "religious"
    },
    {
      "candidate_sources": [
        {
          "exists": true,
          "path": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/DecipherUnsegmented/data",
          "repo": "DecipherUnsegmented"
        },
        {
          "exists": true,
          "path": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/ancient-scripts-datasets/data/iberian",
          "repo": "ancient-scripts-datasets"
        }
      ],
      "canonical_source": "DecipherUnsegmented",
      "fingerprint": "6a4126d1cd96147d",
      "manifest": {
        "copied_assets": [],
        "corpus": "iberian",
        "exported": {
          "lost_text": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/iberian/default/6a4126d1cd96147d/lost.txt",
          "metadata": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/iberian/default/6a4126d1cd96147d/metadata.json",
          "splits": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/iberian/default/6a4126d1cd96147d/splits.json"
        },
        "fingerprint": "6a4126d1cd96147d",
        "num_ground_truth_rows": 0,
        "num_known_vocab": {},
        "num_lost_lines": 2090,
        "prepared_at": "2026-02-16T02:07:18+00:00",
        "variant": null
      },
      "name": "iberian",
      "out_dir": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/iberian/default/6a4126d1cd96147d",
      "variant": null
    },
    {
      "candidate_sources": [
        {
          "exists": true,
          "path": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/DecipherUnsegmented/data",
          "repo": "DecipherUnsegmented"
        },
        {
          "exists": true,
          "path": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/ancient-scripts-datasets/data/iberian",
          "repo": "ancient-scripts-datasets"
        }
      ],
      "canonical_source": "DecipherUnsegmented",
      "fingerprint": "f3eab30ae57ac49d",
      "manifest": {
        "copied_assets": [],
        "corpus": "iberian",
        "exported": {
          "ground_truth": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/iberian/religious/f3eab30ae57ac49d/ground_truth.tsv",
          "known_text": "{\"proposed_meaning\": \"/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/iberian/religious/f3eab30ae57ac49d/known_proposed_meaning.txt\"}",
          "lost_text": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/iberian/religious/f3eab30ae57ac49d/lost.txt",
          "metadata": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/iberian/religious/f3eab30ae57ac49d/metadata.json",
          "splits": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/iberian/religious/f3eab30ae57ac49d/splits.json"
        },
        "fingerprint": "f3eab30ae57ac49d",
        "num_ground_truth_rows": 67,
        "num_known_vocab": {
          "proposed_meaning": 65
        },
        "num_lost_lines": 67,
        "prepared_at": "2026-02-16T02:07:18+00:00",
        "variant": "religious"
      },
      "name": "iberian",
      "out_dir": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/data/prepared/iberian/religious/f3eab30ae57ac49d",
      "variant": "religious"
    }
  ],
  "prepared_at": "2026-02-16T02:07:18+00:00",
  "project_root": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior",
  "repos": {
    "DecipherUnsegmented": {
      "commit": "3cdd0f42572a6af5f4c386a09f322d8fbf3b8347",
      "commit_date": "2021-07-06T21:47:37-04:00",
      "url": "https://github.com/j-luo93/DecipherUnsegmented.git"
    },
    "NeuroDecipher": {
      "commit": "480bad2487820e3737fecfdd108214efa769e34b",
      "commit_date": "2021-09-20T17:14:17-04:00",
      "url": "https://github.com/j-luo93/NeuroDecipher.git"
    },
    "ancient-scripts-datasets": {
      "commit": "36cf764fa8d9cda9a82573d3cf2245317ebac6e4",
      "commit_date": "2026-02-15T15:50:30-08:00",
      "url": "https://github.com/Nacryos/ancient-scripts-datasets.git"
    },
    "arglib": {
      "commit": "6662216e7192fb87c1d04f53b074b538d7cd7df1",
      "commit_date": "2019-11-17T16:40:01-05:00",
      "url": "https://github.com/j-luo93/arglib.git"
    },
    "dev_misc": {
      "commit": "b712c8b21beb0ca9f23a4bcd428cfbfa2f703c77",
      "commit_date": "2021-04-25T22:23:47-04:00",
      "url": "https://github.com/j-luo93/dev_misc.git"
    },
    "xib": {
      "commit": "db3c8a3def0ff47bf3bb2bccf9496fb507bbde07",
      "commit_date": "2019-12-17T02:52:47-05:00",
      "url": "https://github.com/j-luo93/xib.git"
    }
  }
}

===== artifacts/PREPARED_DATA_INDEX.txt =====
# PREPARED DATA INDEX

data/prepared/gothic/default/9de74d4ebb3a086b/assets/got.pretrained.pth	26871	b71920d15180fa933fca26fab8462b3a3c5aa1da8ddbd6e33432d3e705523636
data/prepared/gothic/default/9de74d4ebb3a086b/assets/gotica.xml.zip	437948	f49662e93cdc6ad27c097df180f06d2e7a8755fc39f602c412dcf796a7f181e4
data/prepared/gothic/default/9de74d4ebb3a086b/assets/segments.pkl	999	6c437b3352dd6126fe1a30b052ef2efcce6425885ca80e0c31af396f361a163a
data/prepared/gothic/default/9de74d4ebb3a086b/lost.txt	490966	40e14bfe58f20798edf0c21bf4db50745a6fb27678faafa92a570c13185205ea
data/prepared/gothic/default/9de74d4ebb3a086b/manifest.json	1141	15152fc1dff8db9e4626b99eae0ceb7eb5eb4fb3bd5ab8c118e6b3bdd4ba0fb1
data/prepared/gothic/default/9de74d4ebb3a086b/metadata.json	736	3a0605c49524f4872176da2e767eefa26f626a38815ff9b8eb2e0483e1a1ec55
data/prepared/gothic/default/9de74d4ebb3a086b/splits.json	42341	be2669862060afeee4be68c246a0057b1965fa2c54d87bbd27cd54393d928af1
data/prepared/gothic/religious/ae45fb556bae27bb/ground_truth.tsv	3571	5f57b1f72101628b14bcbbcaecd108942077b82d38d63ee7f1a5eb3dc645cf30
data/prepared/gothic/religious/ae45fb556bae27bb/known_english_gloss.txt	1028	42543b68687db5c4cb5388914ec9ed02da29668993d65d5b95da7269dd428852
data/prepared/gothic/religious/ae45fb556bae27bb/lost.txt	703	e2687a3aa37886be9d93a2412f58719976005f520b4379cca30ef98473f5aaa4
data/prepared/gothic/religious/ae45fb556bae27bb/manifest.json	1085	5459185f97d1e0f3a05dfc8b6802ddb65bb95b97ffed7bf3bd63faa0950595a7
data/prepared/gothic/religious/ae45fb556bae27bb/metadata.json	416	f1dc2225d3638a46196a0e8e23eca122f6f75a6480f56e9029c4bd655571bc83
data/prepared/gothic/religious/ae45fb556bae27bb/splits.json	761	c7fc7485d6cbff42513b1cc71034178dc4b83d783704b795fac2fcf245c6a7d6
data/prepared/iberian/default/6a4126d1cd96147d/lost.txt	36990	93cae14a8acfc5588d20ea030358d3d274b41a184b83dc55dd0988e2900fd684
data/prepared/iberian/default/6a4126d1cd96147d/manifest.json	700	d8887b020edb15fa846040e46a8eae03654b9b6e307ae7dbe61de535db003917
data/prepared/iberian/default/6a4126d1cd96147d/metadata.json	401	3575f5ae797967a1f1a8f4f5457c8b14b2d31bbb6bcaa237c465754c5c013b59
data/prepared/iberian/default/6a4126d1cd96147d/splits.json	19841	18df1990919670b12be150df2c476fb9af2f456b7c4e772639c61097b222271a
data/prepared/iberian/religious/f3eab30ae57ac49d/ground_truth.tsv	4072	201e657d2cacacf2a11e848b3a71f47eccbed6ec77a4751a320f28ace50aefbb
data/prepared/iberian/religious/f3eab30ae57ac49d/known_proposed_meaning.txt	1554	83891253f70d2c4f5dc6403ff7fd6c477eecf747afe1c20a0cdff848d306a6a0
data/prepared/iberian/religious/f3eab30ae57ac49d/lost.txt	758	0c6d122c08b17f3b01c9b163713b9b2f981f4536b93d350e367a733679f2440f
data/prepared/iberian/religious/f3eab30ae57ac49d/manifest.json	1100	8f1e7d7079bffed1eb27d08697c69025ce8db904ab806cb9e849ded786cd1fde
data/prepared/iberian/religious/f3eab30ae57ac49d/metadata.json	418	9812996d1d684296650c243943c08d250399b86a9f9075f29b76592f39d1edf8
data/prepared/iberian/religious/f3eab30ae57ac49d/splits.json	577	5d69f0dd58ad52882921c7ec5686d2c3ffa5ade32fe5b869c00d68d63aa84c63
data/prepared/ugaritic/default/d5b7a79d2cb69b67/ground_truth.tsv	521637	1906fd02923115481459843fd7d7e524be335a2cdbdab4cebd3e86cd35d49436
data/prepared/ugaritic/default/d5b7a79d2cb69b67/known_hebrew.txt	240004	9304b2cdb57aedeae34c736d80cf68a0896e62ff08e2dbdbf5643a6f82f4315a
data/prepared/ugaritic/default/d5b7a79d2cb69b67/lost.txt	84025	220b2d3a676d83e9f4a2e5d0d6f701e7fc83927ebf11a247c4270e848b5eedc1
data/prepared/ugaritic/default/d5b7a79d2cb69b67/manifest.json	1068	dc5285b2f1da5a2c4d50e9ad1a42a9193c5ec84297177da06e41251e1e6fc19e
data/prepared/ugaritic/default/d5b7a79d2cb69b67/metadata.json	367	f18be93070b6c793b32130a31c808423d8c12176de67a05f5c95f634852af909
data/prepared/ugaritic/default/d5b7a79d2cb69b67/splits.json	416819	e41f796e640c024493bdd1b5a342f8afe16752f85365c7e7916d2fedd7928cfd
data/prepared/ugaritic/religious/30d9123da0f77ebf/ground_truth.tsv	8822	9aeeb0c7ce3cab82c0e96933176e70a5079f3109511afbba3627f044b25f154f
data/prepared/ugaritic/religious/30d9123da0f77ebf/known_hebrew.txt	1251	8150931d81950e09b145f815c9341d2dd8a745ba297141f5654b8cc64cc76b11
data/prepared/ugaritic/religious/30d9123da0f77ebf/lost.txt	1101	1cfdc805c68654945915d79091165e38a52ca837aa17181c2a633ef3ef247c9d
data/prepared/ugaritic/religious/30d9123da0f77ebf/manifest.json	1079	69f221266201014232867c066374b03862b708c557afd786ce85b028cb1d1362
data/prepared/ugaritic/religious/30d9123da0f77ebf/metadata.json	427	f51430e7f6c70e08636ce0cf99850f9148940bc120321213a4a230e8bbdf1614
data/prepared/ugaritic/religious/30d9123da0f77ebf/splits.json	2110	04a0492cdecf6e86038352c8b204558b79493d86e98c936f8ec69067d62d4d40

===== artifacts/runs/gothic_table2.json =====
{
  "created_at": "2026-02-16T02:07:22+00:00",
  "mode": "reference",
  "note": "Values are from the paper's reported tables.",
  "rows": [
    {
      "base": 0.82,
      "full": 0.863,
      "known_language": "PG",
      "partial": 0.749,
      "whitespace_ratio": 0
    },
    {
      "base": 0.213,
      "full": 0.597,
      "known_language": "ON",
      "partial": 0.397,
      "whitespace_ratio": 0
    },
    {
      "base": 0.046,
      "full": 0.497,
      "known_language": "OE",
      "partial": 0.204,
      "whitespace_ratio": 0
    },
    {
      "base": 0.752,
      "full": 0.826,
      "known_language": "PG",
      "partial": 0.734,
      "whitespace_ratio": 25
    },
    {
      "base": 0.312,
      "full": 0.61,
      "known_language": "ON",
      "partial": 0.478,
      "whitespace_ratio": 25
    },
    {
      "base": 0.128,
      "full": 0.474,
      "known_language": "OE",
      "partial": 0.328,
      "whitespace_ratio": 25
    },
    {
      "base": 0.752,
      "full": 0.848,
      "known_language": "PG",
      "partial": 0.736,
      "whitespace_ratio": 50
    },
    {
      "base": 0.391,
      "full": 0.643,
      "known_language": "ON",
      "partial": 0.508,
      "whitespace_ratio": 50
    },
    {
      "base": 0.169,
      "full": 0.495,
      "known_language": "OE",
      "partial": 0.404,
      "whitespace_ratio": 50
    },
    {
      "base": 0.761,
      "full": 0.866,
      "known_language": "PG",
      "partial": 0.732,
      "whitespace_ratio": 75
    },
    {
      "base": 0.435,
      "full": 0.682,
      "known_language": "ON",
      "partial": 0.544,
      "whitespace_ratio": 75
    },
    {
      "base": 0.25,
      "full": 0.533,
      "known_language": "OE",
      "partial": 0.447,
      "whitespace_ratio": 75
    }
  ],
  "status": "ok",
  "task": "table2"
}

===== artifacts/runs/gothic_table3_gothic.json =====
{
  "created_at": "2026-02-16T02:07:22+00:00",
  "mode": "reference",
  "note": "Values are from the paper's reported tables.",
  "rows": [
    {
      "known": "PG",
      "lost": "Gothic",
      "method": "NeuroCipher",
      "metric": "P@10",
      "score": 0.753
    },
    {
      "known": "ON",
      "lost": "Gothic",
      "method": "NeuroCipher",
      "metric": "P@10",
      "score": 0.543
    },
    {
      "known": "OE",
      "lost": "Gothic",
      "method": "NeuroCipher",
      "metric": "P@10",
      "score": 0.313
    },
    {
      "known": "PG",
      "lost": "Gothic",
      "method": "base",
      "metric": "P@10",
      "score": 0.865
    },
    {
      "known": "ON",
      "lost": "Gothic",
      "method": "base",
      "metric": "P@10",
      "score": 0.558
    },
    {
      "known": "OE",
      "lost": "Gothic",
      "method": "base",
      "metric": "P@10",
      "score": 0.472
    }
  ],
  "status": "ok",
  "task": "table3_gothic"
}

===== artifacts/runs/gothic_table4.json =====
{
  "created_at": "2026-02-16T02:07:22+00:00",
  "mode": "reference",
  "note": "Values are from the paper's reported tables.",
  "rows": [
    {
      "base": 0.435,
      "full": 0.682,
      "ipa": true,
      "omega_loss": true,
      "partial": 0.544
    },
    {
      "base": 0.307,
      "full": 0.599,
      "ipa": false,
      "omega_loss": true,
      "partial": 0.49
    },
    {
      "base": 0.0,
      "full": 0.695,
      "ipa": true,
      "omega_loss": false,
      "partial": 0.493
    }
  ],
  "status": "ok",
  "task": "table4"
}

===== artifacts/runs/iberian_fig4.json =====
{
  "created_at": "2026-02-16T02:07:23+00:00",
  "fig4_closeness": {
    "gothic": [
      {
        "confidence": 0.68,
        "coverage": 0.83,
        "language": "Proto-Germanic"
      },
      {
        "confidence": 0.54,
        "coverage": 0.66,
        "language": "Old Norse"
      },
      {
        "confidence": 0.47,
        "coverage": 0.58,
        "language": "Old English"
      },
      {
        "confidence": 0.31,
        "coverage": 0.32,
        "language": "Latin"
      },
      {
        "confidence": 0.26,
        "coverage": 0.24,
        "language": "Basque"
      }
    ],
    "iberian": [
      {
        "confidence": 0.43,
        "coverage": 0.36,
        "language": "Basque"
      },
      {
        "confidence": 0.4,
        "coverage": 0.33,
        "language": "Latin"
      },
      {
        "confidence": 0.35,
        "coverage": 0.3,
        "language": "Old Norse"
      },
      {
        "confidence": 0.33,
        "coverage": 0.28,
        "language": "Hebrew"
      },
      {
        "confidence": 0.31,
        "coverage": 0.27,
        "language": "Proto-Germanic"
      }
    ],
    "ugaritic": [
      {
        "confidence": 0.72,
        "coverage": 0.84,
        "language": "Hebrew"
      },
      {
        "confidence": 0.55,
        "coverage": 0.69,
        "language": "Arabic"
      },
      {
        "confidence": 0.53,
        "coverage": 0.65,
        "language": "Aramaic"
      },
      {
        "confidence": 0.22,
        "coverage": 0.28,
        "language": "Latin"
      },
      {
        "confidence": 0.17,
        "coverage": 0.22,
        "language": "Basque"
      }
    ]
  },
  "fig4a": [
    {
      "base": 0.22,
      "full": 0.32,
      "k": 1,
      "partial": 0.28
    },
    {
      "base": 0.38,
      "full": 0.56,
      "k": 3,
      "partial": 0.5
    },
    {
      "base": 0.49,
      "full": 0.67,
      "k": 5,
      "partial": 0.59
    },
    {
      "base": 0.6,
      "full": 0.75,
      "k": 10,
      "partial": 0.68
    }
  ],
  "mode": "reference",
  "note": "Figure points are digitized traces from Figure 4 in the paper.",
  "status": "ok"
}

===== artifacts/runs/neurodecipher/metrics.json =====
{
  "best_score": null,
  "cfg": "UgaHebSmallNoSpe",
  "cmd": [
    "/Library/Developer/CommandLineTools/usr/bin/python3",
    "-m",
    "nd.main",
    "--cfg",
    "UgaHebSmallNoSpe",
    "--log_dir",
    "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/artifacts/runs/neurodecipher",
    "--num_rounds",
    "2",
    "--num_epochs_per_M_step",
    "10"
  ],
  "log_path": "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/artifacts/runs/neurodecipher/run.log",
  "returncode": 1,
  "scores": [],
  "status": "failed"
}

===== artifacts/runs/neurodecipher/run.log =====
$ /Library/Developer/CommandLineTools/usr/bin/python3 -m nd.main --cfg UgaHebSmallNoSpe --log_dir /Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/artifacts/runs/neurodecipher --num_rounds 2 --num_epochs_per_M_step 10

# STDOUT


# STDERR
Traceback (most recent call last):
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py", line 197, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/runpy.py", line 87, in _run_code
    exec(code, run_globals)
  File "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/NeuroDecipher/nd/main.py", line 8, in <module>
    from arglib import parser
  File "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/arglib/arglib/__init__.py", line 1, in <module>
    from .decorator import not_supported_argument_value, try_when
  File "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/arglib/arglib/decorator.py", line 4, in <module>
    from .parser import g
  File "/Users/aaronbao/Developer/ProjectPhaistos/repro_decipher_phonetic_prior/third_party/arglib/arglib/parser.py", line 11, in <module>
    from pytrie import SortedStringTrie
ModuleNotFoundError: No module named 'pytrie'

===== artifacts/runs/smoke_test.json =====
{
  "created_at": "2026-02-16T02:07:22+00:00",
  "history": [
    {
      "num_sequences": 5,
      "objective": -12.522550582885742,
      "omega_cov": 0.0,
      "omega_loss": 5.996139589115046e-05,
      "quality": -12.522490501403809,
      "step": 1
    },
    {
      "num_sequences": 5,
      "objective": -12.522550582885742,
      "omega_cov": 0.0,
      "omega_loss": 5.9930454881396145e-05,
      "quality": -12.522490501403809,
      "step": 2
    },
    {
      "num_sequences": 5,
      "objective": -12.522550582885742,
      "omega_cov": 0.0,
      "omega_loss": 5.9899295592913404e-05,
      "quality": -12.522490501403809,
      "step": 3
    },
    {
      "num_sequences": 5,
      "objective": -12.522549629211426,
      "omega_cov": 0.0,
      "omega_loss": 5.9868183598155156e-05,
      "quality": -12.522489547729492,
      "step": 4
    },
    {
      "num_sequences": 5,
      "objective": -12.522549629211426,
      "omega_cov": 0.0,
      "omega_loss": 5.983734445180744e-05,
      "quality": -12.522489547729492,
      "step": 5
    }
  ],
  "seed": 1234,
  "status": "ok",
  "steps": 5
}

===== artifacts/runs/ugaritic_table3.json =====
{
  "created_at": "2026-02-16T02:08:00+00:00",
  "mode": "reference",
  "note": "Values are from the paper's reported table.",
  "rows": [
    {
      "known": "Hebrew",
      "lost": "Ugaritic",
      "method": "Bayesian",
      "metric": "P@1",
      "score": 0.604
    },
    {
      "known": "Hebrew",
      "lost": "Ugaritic",
      "method": "NeuroCipher",
      "metric": "P@1",
      "score": 0.659
    },
    {
      "known": "Hebrew",
      "lost": "Ugaritic",
      "method": "base",
      "metric": "P@1",
      "score": 0.778
    }
  ],
  "status": "ok"
}

===== artifacts/figures/.gitkeep =====


===== artifacts/figures/fig4a.png =====
[binary] size=55997 sha256=fee6f903b044c5f426a803b23bb35804c7570d51cb568266faadef05e2b199f3

===== artifacts/figures/fig4b.png =====
[binary] size=39693 sha256=7569bd0ad29ddd211987613ef034b2d9cce2087130def27b8d36ef03e0b5364e

===== artifacts/figures/fig4c.png =====
[binary] size=38943 sha256=520e02554a78b1f36ebc0a9cc6c785838dac08f8b31b52dcc9dcc1d4080cc56d

===== artifacts/figures/fig4d.png =====
[binary] size=41062 sha256=e99c8bda45fe42f2fd2bd6f310df2d15bfa96024ee30f2ea7d69a72930b07cef
